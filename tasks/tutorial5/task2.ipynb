{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rds/general/user/oa321/home/miniforge3/envs/mase_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTokenizing dataset imdb with AutoTokenizer for bert-base-uncased.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from chop.tools import get_tokenized_dataset\n",
    "\n",
    "checkpoint = \"prajjwal1/bert-tiny\"\n",
    "tokenizer_checkpoint = \"bert-base-uncased\"\n",
    "dataset_name = \"imdb\"\n",
    "\n",
    "dataset, tokenizer = get_tokenized_dataset(\n",
    "    dataset=dataset_name,\n",
    "    checkpoint=tokenizer_checkpoint,\n",
    "    return_tokenizer=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rds/general/user/oa321/home/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "WARNING:accelerate.utils.other:Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:40, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.693800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.637500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.464200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.404200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.373000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.378700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:21]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.84448\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoModelForSequenceClassification\n",
    "from chop.tools import get_trainer\n",
    "\n",
    "config = AutoConfig.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_config(config)\n",
    "\n",
    "trainer = get_trainer(\n",
    "    model = model,\n",
    "    tokenized_dataset = dataset,\n",
    "    tokenizer = tokenizer,\n",
    "    evaluate_metric = \"accuracy\",\n",
    "    num_train_epochs = 1\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "baseline = eval_results[\"eval_accuracy\"]\n",
    "print(baseline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from chop.nn.modules import Identity\n",
    "\n",
    "search_space = {\n",
    "    \"num_layers\": [2, 4, 8],\n",
    "    \"num_heads\": [2, 4, 8, 16],\n",
    "    \"hidden_size\": [128, 192, 256, 384, 512],\n",
    "    \"intermediate_size\": [512, 768, 1024, 1536, 2048],\n",
    "    \"linear_layer_choices\" : [\"linear\", \"identity\"]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModelForSequenceClassification\n",
    "from chop.tools.utils import deepsetattr\n",
    "\n",
    "track = set({})\n",
    "\n",
    "def construct_model(trial):\n",
    "    config = AutoConfig.from_pretrained(checkpoint)\n",
    "    ss_template = search_space\n",
    "\n",
    "    for param in [\n",
    "        \"num_layers\",\n",
    "        \"num_heads\",\n",
    "        \"hidden_size\",\n",
    "        \"intermediate_size\"\n",
    "    ]:\n",
    "        chosen_idex = trial.suggest_int(param, 0, len(ss_template[param]) - 1)\n",
    "        setattr(config, param, ss_template[param][chosen_idex])\n",
    "\n",
    "    trial_model = AutoModelForSequenceClassification.from_config(config)\n",
    "\n",
    "    for name, layer in trial_model.named_modules():\n",
    "        if isinstance(layer, nn.Linear) and layer.in_features == layer.out_features:\n",
    "            track.add(name)\n",
    "            new_layer_cls = trial.suggest_categorical(\n",
    "                f\"{name}_type\",\n",
    "                ss_template[\"linear_layer_choices\"],\n",
    "            )\n",
    "\n",
    "            if new_layer_cls == \"linear\":\n",
    "                continue\n",
    "            elif new_layer_cls == \"identity\":\n",
    "                new_layer = Identity()\n",
    "                deepsetattr(trial_model, name, new_layer)\n",
    "            else:\n",
    "                raise ValueError(f\"Unkown layer type: {new_layer_cls}\")\n",
    "\n",
    "    return trial_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chop.tools import get_trainer\n",
    "\n",
    "def objective(trial):\n",
    "    model = construct_model(trial)\n",
    "\n",
    "    trainer = get_trainer(\n",
    "        model = model,\n",
    "        tokenized_dataset = dataset,\n",
    "        tokenizer = tokenizer,\n",
    "        evaluate_metric = \"accuracy\",\n",
    "        num_train_epochs = 1\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    eval_results = trainer.evaluate()\n",
    "\n",
    "    trial.set_user_attr(\"model\", model)\n",
    "\n",
    "    return eval_results[\"eval_accuracy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.samplers import GridSampler, RandomSampler, TPESampler\n",
    "\n",
    "def getGridSearchSpace():\n",
    "    grid_search_space = {\n",
    "        \"num_layers\": [i for i in range(0, len(search_space['num_layers']))],\n",
    "        \"num_heads\": [i for i in range(0, len(search_space['num_heads']))],\n",
    "        \"hidden_size\": [i for i in range(0, len(search_space['hidden_size']))],\n",
    "        \"intermediate_size\": [i for i in range(0, len(search_space['intermediate_size']))],\n",
    "    }\n",
    "\n",
    "    sampler = RandomSampler()\n",
    "\n",
    "    study = optuna.create_study(\n",
    "        direction=\"maximize\",\n",
    "        study_name=\"bert-tiny-nas-study\",\n",
    "        sampler=sampler\n",
    "    )\n",
    "\n",
    "    study.optimize(\n",
    "        lambda trial: construct_model(trial=trial),\n",
    "        n_trials = 100,\n",
    "        timeout=60*60*24\n",
    "    )\n",
    "\n",
    "    for name in track :\n",
    "        grid_search_space[f'{name}_type'] = [\"linear\", \"identity\"]\n",
    "\n",
    "    return grid_search_space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampler = GridSampler(search_space=getGridSearchSpace()) # The sampler to use below\n",
    "sampler = RandomSampler()\n",
    "# sampler = TPESampler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-02 22:29:05,475] A new study created in memory with name: bert-tiny-nas-study\n",
      "/rds/general/user/oa321/home/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "WARNING:accelerate.utils.other:Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 01:01, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.651100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.478600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.416100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.366000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.333200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.340600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:26]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-02 22:30:34,467] Trial 0 finished with value: 0.86304 and parameters: {'num_layers': 2, 'num_heads': 1, 'hidden_size': 3, 'intermediate_size': 2, 'bert.encoder.layer.0.attention.self.query_type': 'linear', 'bert.encoder.layer.0.attention.self.key_type': 'linear', 'bert.encoder.layer.0.attention.self.value_type': 'linear', 'bert.encoder.layer.0.attention.output.dense_type': 'identity', 'bert.encoder.layer.1.attention.self.query_type': 'identity', 'bert.encoder.layer.1.attention.self.key_type': 'linear', 'bert.encoder.layer.1.attention.self.value_type': 'identity', 'bert.encoder.layer.1.attention.output.dense_type': 'identity', 'bert.pooler.dense_type': 'identity'}. Best is trial 0 with value: 0.86304.\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    study_name=\"bert-tiny-nas-study\",\n",
    "    sampler=sampler\n",
    ")\n",
    "\n",
    "study.optimize(\n",
    "    objective,\n",
    "    n_trials = 1,\n",
    "    timeout=60*60*24\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import dill\n",
    "\n",
    "model = study.best_trial.user_attrs[\"model\"].cpu()\n",
    "\n",
    "with open(f\"{Path.home()}/mase/tasks/tutorial5/t5_best_model.pkl\", \"wb\") as f:\n",
    "    dill.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mGetting dummy input for prajjwal1/bert-tiny.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101, 9932, 2089, 2202, 2058, 1996, 2088, 2028, 2154,  102],\n",
      "        [ 101, 2023, 2003, 2339, 2017, 2323, 4553, 4748, 4877,  102]])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tensor([[ 101, 9932, 2089, 2202, 2058, 1996, 2088, 2028, 2154,  102],\n",
      "        [ 101, 2023, 2003, 2339, 2017, 2323, 4553, 4748, 4877,  102]])\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "tensor([[[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]]])\n",
      "tensor([[[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]]])\n",
      "tensor([[[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]])\n",
      "tensor([[[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]])\n",
      "tensor([[[-0.6172,  0.3536,  0.2559,  ...,  0.8533, -0.3603, -0.0813],\n",
      "         [ 0.5967,  1.1123,  0.0158,  ...,  0.3694, -0.7771, -0.9590],\n",
      "         [ 0.1848,  1.5788,  1.3072,  ..., -0.0534, -0.4268, -1.4919],\n",
      "         ...,\n",
      "         [ 0.5694,  0.1506, -0.3463,  ..., -1.1028,  0.8037, -1.1045],\n",
      "         [ 1.0147, -1.4384,  1.4985,  ..., -0.4912,  0.0863,  0.6208],\n",
      "         [ 1.0156, -0.7236,  0.5946,  ..., -0.2067, -0.1649, -0.4109]],\n",
      "\n",
      "        [[-0.6172,  0.3536,  0.2559,  ...,  0.8533, -0.3603, -0.0813],\n",
      "         [ 0.1931,  0.9846,  0.8442,  ...,  0.0609, -1.5716,  0.3447],\n",
      "         [-0.0143, -1.0149,  0.3668,  ...,  1.0366, -0.1178, -2.3161],\n",
      "         ...,\n",
      "         [ 1.5300,  0.8623,  0.5396,  ...,  0.0902, -0.5839, -1.1281],\n",
      "         [ 1.1435, -0.1295, -0.0825,  ...,  1.1138,  0.2275, -1.8479],\n",
      "         [ 1.0156, -0.7236,  0.5946,  ..., -0.2067, -0.1649, -0.4109]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[[ 1.8232, -0.2310, -0.7433,  ..., -0.9594,  0.2381,  1.1260],\n",
      "         [ 0.5998, -0.3345, -0.5021,  ...,  0.5459,  0.1729,  0.7258],\n",
      "         [ 0.5910, -0.1868, -0.7606,  ...,  0.3613,  0.5805,  0.5618],\n",
      "         ...,\n",
      "         [ 0.5796,  0.2383,  0.1929,  ..., -0.0192,  0.2200,  0.7258],\n",
      "         [ 0.6496, -0.6444, -0.5432,  ...,  0.3179,  1.0667,  0.0068],\n",
      "         [ 0.2344, -0.4503, -0.2826,  ...,  0.1466,  0.2689,  1.0488]],\n",
      "\n",
      "        [[ 1.8232, -0.2310, -0.7433,  ..., -0.9594,  0.2381,  1.1260],\n",
      "         [ 1.2136,  0.0269, -0.8408,  ...,  0.9515,  0.5323,  0.6847],\n",
      "         [ 0.6922, -0.2842, -0.5249,  ..., -0.3763,  0.1551,  0.5130],\n",
      "         ...,\n",
      "         [ 0.2027,  0.6917, -0.3924,  ..., -0.1008,  0.3284,  1.0227],\n",
      "         [ 0.7775, -0.8252, -0.4105,  ...,  0.2441,  0.7402,  0.4746],\n",
      "         [ 0.2344, -0.4503, -0.2826,  ...,  0.1466,  0.2689,  1.0488]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[ 1.8232, -0.2310, -0.7433,  ..., -0.9594,  0.2381,  1.1260],\n",
      "         [ 0.5998, -0.3345, -0.5021,  ...,  0.5459,  0.1729,  0.7258],\n",
      "         [ 0.5910, -0.1868, -0.7606,  ...,  0.3613,  0.5805,  0.5618],\n",
      "         ...,\n",
      "         [ 0.5796,  0.2383,  0.1929,  ..., -0.0192,  0.2200,  0.7258],\n",
      "         [ 0.6496, -0.6444, -0.5432,  ...,  0.3179,  1.0667,  0.0068],\n",
      "         [ 0.2344, -0.4503, -0.2826,  ...,  0.1466,  0.2689,  1.0488]],\n",
      "\n",
      "        [[ 1.8232, -0.2310, -0.7433,  ..., -0.9594,  0.2381,  1.1260],\n",
      "         [ 1.2136,  0.0269, -0.8408,  ...,  0.9515,  0.5323,  0.6847],\n",
      "         [ 0.6922, -0.2842, -0.5249,  ..., -0.3763,  0.1551,  0.5130],\n",
      "         ...,\n",
      "         [ 0.2027,  0.6917, -0.3924,  ..., -0.1008,  0.3284,  1.0227],\n",
      "         [ 0.7775, -0.8252, -0.4105,  ...,  0.2441,  0.7402,  0.4746],\n",
      "         [ 0.2344, -0.4503, -0.2826,  ...,  0.1466,  0.2689,  1.0488]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[[ 1.8232e+00, -2.3098e-01, -7.4328e-01,  ...,  5.8661e-01,\n",
      "           -7.4241e-01, -1.2196e+00],\n",
      "          [ 2.6750e-01,  3.3048e-01, -1.0384e+00,  ..., -9.5944e-01,\n",
      "            2.3807e-01,  1.1260e+00]],\n",
      "\n",
      "         [[ 5.9984e-01, -3.3445e-01, -5.0205e-01,  ...,  9.5485e-01,\n",
      "           -4.5230e-02, -1.4576e-01],\n",
      "          [ 6.2776e-01,  7.4434e-02, -4.8766e-01,  ...,  5.4588e-01,\n",
      "            1.7294e-01,  7.2580e-01]],\n",
      "\n",
      "         [[ 5.9103e-01, -1.8683e-01, -7.6059e-01,  ...,  3.8738e-01,\n",
      "           -4.3966e-01, -6.8199e-01],\n",
      "          [ 2.9580e-01,  4.8412e-01, -2.7976e-01,  ...,  3.6133e-01,\n",
      "            5.8049e-01,  5.6182e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 5.7963e-01,  2.3826e-01,  1.9293e-01,  ...,  4.5052e-01,\n",
      "            8.3587e-01, -4.1063e-01],\n",
      "          [-1.5828e-01,  1.5999e-01, -1.0362e+00,  ..., -1.9193e-02,\n",
      "            2.2003e-01,  7.2584e-01]],\n",
      "\n",
      "         [[ 6.4963e-01, -6.4439e-01, -5.4321e-01,  ...,  2.3358e-01,\n",
      "            5.5061e-01, -3.6979e-01],\n",
      "          [ 4.3156e-01,  9.2552e-01,  1.2482e-01,  ...,  3.1790e-01,\n",
      "            1.0667e+00,  6.7602e-03]],\n",
      "\n",
      "         [[ 2.3439e-01, -4.5028e-01, -2.8257e-01,  ...,  3.7926e-02,\n",
      "            1.7009e-01, -6.8091e-01],\n",
      "          [ 6.5911e-01, -2.0626e-02, -1.1421e+00,  ...,  1.4661e-01,\n",
      "            2.6895e-01,  1.0488e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 1.8232e+00, -2.3098e-01, -7.4328e-01,  ...,  5.8661e-01,\n",
      "           -7.4241e-01, -1.2196e+00],\n",
      "          [ 2.6750e-01,  3.3048e-01, -1.0384e+00,  ..., -9.5944e-01,\n",
      "            2.3807e-01,  1.1260e+00]],\n",
      "\n",
      "         [[ 1.2136e+00,  2.6921e-02, -8.4076e-01,  ...,  1.1968e-01,\n",
      "           -1.9211e-01,  3.4501e-01],\n",
      "          [ 4.8129e-01,  1.8789e-01,  1.3500e-05,  ...,  9.5152e-01,\n",
      "            5.3233e-01,  6.8465e-01]],\n",
      "\n",
      "         [[ 6.9217e-01, -2.8417e-01, -5.2492e-01,  ...,  2.8821e-01,\n",
      "            6.2601e-02, -8.2600e-01],\n",
      "          [ 1.7048e-01,  1.5980e-01, -2.7544e-01,  ..., -3.7628e-01,\n",
      "            1.5508e-01,  5.1302e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.0270e-01,  6.9165e-01, -3.9236e-01,  ...,  5.2479e-01,\n",
      "            5.2244e-01, -1.4854e-01],\n",
      "          [-6.1254e-01,  3.9602e-01, -1.2478e+00,  ..., -1.0082e-01,\n",
      "            3.2838e-01,  1.0227e+00]],\n",
      "\n",
      "         [[ 7.7747e-01, -8.2521e-01, -4.1051e-01,  ...,  1.3534e-02,\n",
      "            6.6237e-01, -4.8323e-01],\n",
      "          [ 7.0290e-01,  7.1806e-01, -5.7690e-01,  ...,  2.4414e-01,\n",
      "            7.4023e-01,  4.7461e-01]],\n",
      "\n",
      "         [[ 2.3439e-01, -4.5028e-01, -2.8257e-01,  ...,  3.7926e-02,\n",
      "            1.7009e-01, -6.8091e-01],\n",
      "          [ 6.5911e-01, -2.0626e-02, -1.1421e+00,  ...,  1.4661e-01,\n",
      "            2.6895e-01,  1.0488e+00]]]], grad_fn=<ViewBackward0>)\n",
      "tensor([[[ 0.6712,  0.0558, -0.1641,  ..., -0.0364, -0.3758, -0.5837],\n",
      "         [ 0.6057,  0.2855,  0.2650,  ..., -0.2340, -0.2024,  0.0104],\n",
      "         [ 1.1693, -0.3313,  0.9738,  ..., -0.1116,  0.4444, -0.2151],\n",
      "         ...,\n",
      "         [-0.2076,  0.3276,  0.3017,  ..., -0.5158,  0.2280, -0.2938],\n",
      "         [ 0.8465,  0.2191, -0.1822,  ..., -0.4242,  0.5563,  0.3151],\n",
      "         [-0.1810,  0.0131,  0.2369,  ...,  0.0357,  0.0500, -0.7055]],\n",
      "\n",
      "        [[ 0.6712,  0.0558, -0.1641,  ..., -0.0364, -0.3758, -0.5837],\n",
      "         [ 0.6918,  0.0244, -0.2987,  ..., -0.1327, -0.1083,  0.3170],\n",
      "         [ 0.3736,  0.1032,  0.9263,  ..., -0.5593,  0.1975, -0.3334],\n",
      "         ...,\n",
      "         [ 0.2047,  0.2969,  0.0764,  ..., -0.0441, -0.2348, -0.3490],\n",
      "         [-0.2893,  0.4037,  0.0098,  ..., -0.0440,  0.4682,  0.3439],\n",
      "         [-0.1810,  0.0131,  0.2369,  ...,  0.0357,  0.0500, -0.7055]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[ 0.6712,  0.0558, -0.1641,  ..., -0.0364, -0.3758, -0.5837],\n",
      "         [ 0.6057,  0.2855,  0.2650,  ..., -0.2340, -0.2024,  0.0104],\n",
      "         [ 1.1693, -0.3313,  0.9738,  ..., -0.1116,  0.4444, -0.2151],\n",
      "         ...,\n",
      "         [-0.2076,  0.3276,  0.3017,  ..., -0.5158,  0.2280, -0.2938],\n",
      "         [ 0.8465,  0.2191, -0.1822,  ..., -0.4242,  0.5563,  0.3151],\n",
      "         [-0.1810,  0.0131,  0.2369,  ...,  0.0357,  0.0500, -0.7055]],\n",
      "\n",
      "        [[ 0.6712,  0.0558, -0.1641,  ..., -0.0364, -0.3758, -0.5837],\n",
      "         [ 0.6918,  0.0244, -0.2987,  ..., -0.1327, -0.1083,  0.3170],\n",
      "         [ 0.3736,  0.1032,  0.9263,  ..., -0.5593,  0.1975, -0.3334],\n",
      "         ...,\n",
      "         [ 0.2047,  0.2969,  0.0764,  ..., -0.0441, -0.2348, -0.3490],\n",
      "         [-0.2893,  0.4037,  0.0098,  ..., -0.0440,  0.4682,  0.3439],\n",
      "         [-0.1810,  0.0131,  0.2369,  ...,  0.0357,  0.0500, -0.7055]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[[ 0.6712,  0.0558, -0.1641,  ...,  0.5345,  0.5241,  0.6824],\n",
      "          [ 0.0928, -0.4409, -0.9690,  ..., -0.0364, -0.3758, -0.5837]],\n",
      "\n",
      "         [[ 0.6057,  0.2855,  0.2650,  ...,  0.0354,  0.0064,  0.3826],\n",
      "          [-0.1622,  0.4358, -0.3325,  ..., -0.2340, -0.2024,  0.0104]],\n",
      "\n",
      "         [[ 1.1693, -0.3313,  0.9738,  ...,  0.5667,  0.3827,  0.0714],\n",
      "          [ 1.2178,  0.1005, -0.1628,  ..., -0.1116,  0.4444, -0.2151]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.2076,  0.3276,  0.3017,  ...,  0.2372, -0.0119,  0.5015],\n",
      "          [ 0.2077,  0.4651, -0.3214,  ..., -0.5158,  0.2280, -0.2938]],\n",
      "\n",
      "         [[ 0.8465,  0.2191, -0.1822,  ...,  0.0697, -0.5180,  0.0427],\n",
      "          [ 0.6622,  0.5359, -0.5988,  ..., -0.4242,  0.5563,  0.3151]],\n",
      "\n",
      "         [[-0.1810,  0.0131,  0.2369,  ...,  0.1304,  0.1227,  0.2224],\n",
      "          [ 0.3215,  0.0526,  0.0972,  ...,  0.0357,  0.0500, -0.7055]]],\n",
      "\n",
      "\n",
      "        [[[ 0.6712,  0.0558, -0.1641,  ...,  0.5345,  0.5241,  0.6824],\n",
      "          [ 0.0928, -0.4409, -0.9690,  ..., -0.0364, -0.3758, -0.5837]],\n",
      "\n",
      "         [[ 0.6918,  0.0244, -0.2987,  ...,  0.4303,  0.4558,  0.1783],\n",
      "          [ 0.1206,  0.3919, -0.4695,  ..., -0.1327, -0.1083,  0.3170]],\n",
      "\n",
      "         [[ 0.3736,  0.1032,  0.9263,  ...,  0.4681,  0.2199,  0.2867],\n",
      "          [ 0.5339,  0.4248, -0.3244,  ..., -0.5593,  0.1975, -0.3334]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.2047,  0.2969,  0.0764,  ...,  0.3027,  0.0536,  0.4146],\n",
      "          [ 0.2446,  0.2377, -0.6575,  ..., -0.0441, -0.2348, -0.3490]],\n",
      "\n",
      "         [[-0.2893,  0.4037,  0.0098,  ...,  0.5652, -0.0155, -0.0378],\n",
      "          [ 0.8857,  0.4613, -0.2152,  ..., -0.0440,  0.4682,  0.3439]],\n",
      "\n",
      "         [[-0.1810,  0.0131,  0.2369,  ...,  0.1304,  0.1227,  0.2224],\n",
      "          [ 0.3215,  0.0526,  0.0972,  ...,  0.0357,  0.0500, -0.7055]]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[-0.0623, -0.2020, -0.4023,  ..., -0.0454, -0.2719,  0.5100],\n",
      "         [ 0.2670, -0.0189,  0.0825,  ...,  0.1767,  0.3971,  0.4066],\n",
      "         [-0.6526, -0.0995,  0.1166,  ..., -0.0793,  0.6634,  0.3350],\n",
      "         ...,\n",
      "         [ 0.1668, -0.2297, -0.4293,  ...,  0.2718,  0.2371,  0.5390],\n",
      "         [-0.1954, -0.5609, -0.1586,  ..., -0.7918,  0.7384,  1.0030],\n",
      "         [-0.1084, -0.7293, -0.0353,  ..., -0.5739,  0.1882,  0.3290]],\n",
      "\n",
      "        [[-0.0623, -0.2020, -0.4023,  ..., -0.0454, -0.2719,  0.5100],\n",
      "         [ 0.0868,  0.2431,  0.0817,  ...,  0.2389,  0.2181,  0.1718],\n",
      "         [-0.0219, -0.4024,  0.4181,  ..., -0.9434,  0.4832,  0.2171],\n",
      "         ...,\n",
      "         [ 0.2396, -0.6487, -0.3173,  ..., -0.1628,  0.2172,  0.6986],\n",
      "         [-0.4038, -0.1888,  0.0992,  ..., -0.5986,  0.3410,  0.4988],\n",
      "         [-0.1084, -0.7293, -0.0353,  ..., -0.5739,  0.1882,  0.3290]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[-0.0623, -0.2020, -0.4023,  ..., -0.0454, -0.2719,  0.5100],\n",
      "         [ 0.2670, -0.0189,  0.0825,  ...,  0.1767,  0.3971,  0.4066],\n",
      "         [-0.6526, -0.0995,  0.1166,  ..., -0.0793,  0.6634,  0.3350],\n",
      "         ...,\n",
      "         [ 0.1668, -0.2297, -0.4293,  ...,  0.2718,  0.2371,  0.5390],\n",
      "         [-0.1954, -0.5609, -0.1586,  ..., -0.7918,  0.7384,  1.0030],\n",
      "         [-0.1084, -0.7293, -0.0353,  ..., -0.5739,  0.1882,  0.3290]],\n",
      "\n",
      "        [[-0.0623, -0.2020, -0.4023,  ..., -0.0454, -0.2719,  0.5100],\n",
      "         [ 0.0868,  0.2431,  0.0817,  ...,  0.2389,  0.2181,  0.1718],\n",
      "         [-0.0219, -0.4024,  0.4181,  ..., -0.9434,  0.4832,  0.2171],\n",
      "         ...,\n",
      "         [ 0.2396, -0.6487, -0.3173,  ..., -0.1628,  0.2172,  0.6986],\n",
      "         [-0.4038, -0.1888,  0.0992,  ..., -0.5986,  0.3410,  0.4988],\n",
      "         [-0.1084, -0.7293, -0.0353,  ..., -0.5739,  0.1882,  0.3290]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[[-0.0623, -0.2020, -0.4023,  ..., -0.3341, -0.2050, -0.3063],\n",
      "          [-0.2782,  0.3305,  0.6901,  ..., -0.0454, -0.2719,  0.5100]],\n",
      "\n",
      "         [[ 0.2670, -0.0189,  0.0825,  ..., -0.3693, -0.1665, -0.1061],\n",
      "          [-0.9423,  0.2026,  0.2613,  ...,  0.1767,  0.3971,  0.4066]],\n",
      "\n",
      "         [[-0.6526, -0.0995,  0.1166,  ...,  0.1209, -0.3017, -0.7169],\n",
      "          [-0.9204,  0.1177,  0.3741,  ..., -0.0793,  0.6634,  0.3350]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.1668, -0.2297, -0.4293,  ..., -0.4087, -0.5396, -0.5033],\n",
      "          [-0.2955,  0.2352,  0.2469,  ...,  0.2718,  0.2371,  0.5390]],\n",
      "\n",
      "         [[-0.1954, -0.5609, -0.1586,  ..., -0.6596,  0.3167, -0.0333],\n",
      "          [ 0.2079,  0.3872, -0.2555,  ..., -0.7918,  0.7384,  1.0030]],\n",
      "\n",
      "         [[-0.1084, -0.7293, -0.0353,  ..., -0.3575, -0.5088, -0.2851],\n",
      "          [-0.6738, -0.0200,  0.5046,  ..., -0.5739,  0.1882,  0.3290]]],\n",
      "\n",
      "\n",
      "        [[[-0.0623, -0.2020, -0.4023,  ..., -0.3341, -0.2050, -0.3063],\n",
      "          [-0.2782,  0.3305,  0.6901,  ..., -0.0454, -0.2719,  0.5100]],\n",
      "\n",
      "         [[ 0.0868,  0.2431,  0.0817,  ..., -0.5362, -0.2458, -0.2513],\n",
      "          [-0.6797,  0.2995, -0.0023,  ...,  0.2389,  0.2181,  0.1718]],\n",
      "\n",
      "         [[-0.0219, -0.4024,  0.4181,  ..., -0.2920, -0.0337, -0.4609],\n",
      "          [-1.1327,  0.1862, -0.2457,  ..., -0.9434,  0.4832,  0.2171]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.2396, -0.6487, -0.3173,  ..., -0.7262, -0.7061, -0.8639],\n",
      "          [-1.0757,  0.3335,  0.0612,  ..., -0.1628,  0.2172,  0.6986]],\n",
      "\n",
      "         [[-0.4038, -0.1888,  0.0992,  ..., -0.2285, -0.0152, -0.3171],\n",
      "          [ 0.0391,  0.2296,  0.0772,  ..., -0.5986,  0.3410,  0.4988]],\n",
      "\n",
      "         [[-0.1084, -0.7293, -0.0353,  ..., -0.3575, -0.5088, -0.2851],\n",
      "          [-0.6738, -0.0200,  0.5046,  ..., -0.5739,  0.1882,  0.3290]]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[[-0.0642, -0.2363,  0.0281,  ..., -0.3722, -0.1776, -0.2762],\n",
      "          [-0.0557, -0.1571,  0.0378,  ..., -0.2892, -0.3037, -0.3391],\n",
      "          [-0.0715, -0.1985,  0.0260,  ..., -0.3132, -0.2626, -0.3263],\n",
      "          ...,\n",
      "          [-0.0570, -0.1617,  0.0648,  ..., -0.2799, -0.2955, -0.3434],\n",
      "          [-0.0685, -0.1618,  0.0439,  ..., -0.2901, -0.2870, -0.3446],\n",
      "          [-0.0706, -0.1883,  0.0310,  ..., -0.3138, -0.2531, -0.3256]],\n",
      "\n",
      "         [[-0.6019,  0.2526,  0.2571,  ..., -0.1719,  0.2890,  0.3801],\n",
      "          [-0.5845,  0.2448,  0.2653,  ..., -0.1576,  0.2970,  0.4046],\n",
      "          [-0.5868,  0.2319,  0.2859,  ..., -0.1254,  0.2834,  0.4139],\n",
      "          ...,\n",
      "          [-0.5720,  0.2280,  0.2711,  ..., -0.1319,  0.3048,  0.4039],\n",
      "          [-0.5348,  0.2566,  0.2258,  ..., -0.1960,  0.3200,  0.4512],\n",
      "          [-0.5599,  0.2393,  0.2676,  ..., -0.1546,  0.2975,  0.4037]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1298, -0.0331, -0.0433,  ..., -0.3054, -0.2442, -0.3561],\n",
      "          [ 0.0931, -0.0710, -0.0277,  ..., -0.3192, -0.2518, -0.3518],\n",
      "          [ 0.0590, -0.0947,  0.0013,  ..., -0.3310, -0.2531, -0.3588],\n",
      "          ...,\n",
      "          [ 0.0880, -0.0525, -0.0138,  ..., -0.3268, -0.2322, -0.3432],\n",
      "          [ 0.1238, -0.0496, -0.0277,  ..., -0.3268, -0.2496, -0.3389],\n",
      "          [ 0.0741, -0.0816, -0.0236,  ..., -0.3126, -0.2393, -0.3478]],\n",
      "\n",
      "         [[-0.4183,  0.1031,  0.2290,  ..., -0.1388,  0.2556,  0.4135],\n",
      "          [-0.4285,  0.1160,  0.2161,  ..., -0.1735,  0.2556,  0.4196],\n",
      "          [-0.4667,  0.1299,  0.2134,  ..., -0.2118,  0.2594,  0.4242],\n",
      "          ...,\n",
      "          [-0.4257,  0.1132,  0.2097,  ..., -0.1718,  0.2603,  0.4252],\n",
      "          [-0.4055,  0.1150,  0.2237,  ..., -0.1798,  0.2526,  0.4191],\n",
      "          [-0.4488,  0.1064,  0.2144,  ..., -0.1690,  0.2610,  0.4229]]]],\n",
      "       grad_fn=<ScaledDotProductFlashAttentionForCpuBackward0>)\n",
      "tensor([[[[-0.0642, -0.2363,  0.0281,  ..., -0.3722, -0.1776, -0.2762],\n",
      "          [-0.6019,  0.2526,  0.2571,  ..., -0.1719,  0.2890,  0.3801]],\n",
      "\n",
      "         [[-0.0557, -0.1571,  0.0378,  ..., -0.2892, -0.3037, -0.3391],\n",
      "          [-0.5845,  0.2448,  0.2653,  ..., -0.1576,  0.2970,  0.4046]],\n",
      "\n",
      "         [[-0.0715, -0.1985,  0.0260,  ..., -0.3132, -0.2626, -0.3263],\n",
      "          [-0.5868,  0.2319,  0.2859,  ..., -0.1254,  0.2834,  0.4139]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0570, -0.1617,  0.0648,  ..., -0.2799, -0.2955, -0.3434],\n",
      "          [-0.5720,  0.2280,  0.2711,  ..., -0.1319,  0.3048,  0.4039]],\n",
      "\n",
      "         [[-0.0685, -0.1618,  0.0439,  ..., -0.2901, -0.2870, -0.3446],\n",
      "          [-0.5348,  0.2566,  0.2258,  ..., -0.1960,  0.3200,  0.4512]],\n",
      "\n",
      "         [[-0.0706, -0.1883,  0.0310,  ..., -0.3138, -0.2531, -0.3256],\n",
      "          [-0.5599,  0.2393,  0.2676,  ..., -0.1546,  0.2975,  0.4037]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1298, -0.0331, -0.0433,  ..., -0.3054, -0.2442, -0.3561],\n",
      "          [-0.4183,  0.1031,  0.2290,  ..., -0.1388,  0.2556,  0.4135]],\n",
      "\n",
      "         [[ 0.0931, -0.0710, -0.0277,  ..., -0.3192, -0.2518, -0.3518],\n",
      "          [-0.4285,  0.1160,  0.2161,  ..., -0.1735,  0.2556,  0.4196]],\n",
      "\n",
      "         [[ 0.0590, -0.0947,  0.0013,  ..., -0.3310, -0.2531, -0.3588],\n",
      "          [-0.4667,  0.1299,  0.2134,  ..., -0.2118,  0.2594,  0.4242]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0880, -0.0525, -0.0138,  ..., -0.3268, -0.2322, -0.3432],\n",
      "          [-0.4257,  0.1132,  0.2097,  ..., -0.1718,  0.2603,  0.4252]],\n",
      "\n",
      "         [[ 0.1238, -0.0496, -0.0277,  ..., -0.3268, -0.2496, -0.3389],\n",
      "          [-0.4055,  0.1150,  0.2237,  ..., -0.1798,  0.2526,  0.4191]],\n",
      "\n",
      "         [[ 0.0741, -0.0816, -0.0236,  ..., -0.3126, -0.2393, -0.3478],\n",
      "          [-0.4488,  0.1064,  0.2144,  ..., -0.1690,  0.2610,  0.4229]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "tensor([[[-1.1066e+00,  1.3541e-01,  2.8545e-01,  ...,  6.1323e-01,\n",
      "           1.5040e-01,  8.6120e-01],\n",
      "         [ 1.5030e-01,  9.7659e-01, -5.2427e-01,  ..., -1.8798e-01,\n",
      "           2.1091e-01, -1.3520e-01],\n",
      "         [ 1.8113e-01,  1.7759e+00,  1.2599e+00,  ..., -6.6381e-01,\n",
      "           3.0515e-01, -9.4270e-01],\n",
      "         ...,\n",
      "         [ 3.7392e-02, -1.5967e-03, -6.1884e-01,  ..., -1.7571e+00,\n",
      "           1.7540e+00, -3.2654e-01],\n",
      "         [ 8.1910e-01, -1.5669e+00,  1.2085e+00,  ..., -1.1956e+00,\n",
      "           1.0348e+00,  1.3482e+00],\n",
      "         [ 7.5041e-01, -9.3876e-01,  3.3040e-01,  ..., -7.6609e-01,\n",
      "           7.1825e-01,  7.5741e-01]],\n",
      "\n",
      "        [[-8.5695e-01,  3.8930e-01,  2.6206e-01,  ...,  7.1772e-01,\n",
      "           1.9446e-02,  8.1998e-01],\n",
      "         [ 1.6079e-01,  1.2801e+00,  8.6234e-01,  ..., -3.6039e-01,\n",
      "          -9.2365e-01,  9.4000e-01],\n",
      "         [-4.8924e-02, -1.3295e+00, -1.9110e-01,  ...,  3.2695e-01,\n",
      "           8.6042e-01, -1.6152e+00],\n",
      "         ...,\n",
      "         [ 1.6257e+00,  9.6263e-01,  5.2827e-01,  ..., -3.6590e-01,\n",
      "          -1.3925e-01, -7.8353e-01],\n",
      "         [ 1.4429e+00, -6.1786e-02, -1.7952e-01,  ...,  8.7200e-01,\n",
      "           8.4147e-01, -1.4253e+00],\n",
      "         [ 9.4006e-01, -7.5499e-01,  3.5424e-01,  ..., -7.0291e-01,\n",
      "           5.6228e-01,  7.0141e-01]]], grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[[-1.1066e+00,  1.3541e-01,  2.8545e-01,  ...,  6.1323e-01,\n",
      "           1.5040e-01,  8.6120e-01],\n",
      "         [ 1.5030e-01,  9.7659e-01, -5.2427e-01,  ..., -1.8798e-01,\n",
      "           2.1091e-01, -1.3520e-01],\n",
      "         [ 1.8113e-01,  1.7759e+00,  1.2599e+00,  ..., -6.6381e-01,\n",
      "           3.0515e-01, -9.4270e-01],\n",
      "         ...,\n",
      "         [ 3.7392e-02, -1.5967e-03, -6.1884e-01,  ..., -1.7571e+00,\n",
      "           1.7540e+00, -3.2654e-01],\n",
      "         [ 8.1910e-01, -1.5669e+00,  1.2085e+00,  ..., -1.1956e+00,\n",
      "           1.0348e+00,  1.3482e+00],\n",
      "         [ 7.5041e-01, -9.3876e-01,  3.3040e-01,  ..., -7.6609e-01,\n",
      "           7.1825e-01,  7.5741e-01]],\n",
      "\n",
      "        [[-8.5695e-01,  3.8930e-01,  2.6206e-01,  ...,  7.1772e-01,\n",
      "           1.9446e-02,  8.1998e-01],\n",
      "         [ 1.6079e-01,  1.2801e+00,  8.6234e-01,  ..., -3.6039e-01,\n",
      "          -9.2365e-01,  9.4000e-01],\n",
      "         [-4.8924e-02, -1.3295e+00, -1.9110e-01,  ...,  3.2695e-01,\n",
      "           8.6042e-01, -1.6152e+00],\n",
      "         ...,\n",
      "         [ 1.6257e+00,  9.6263e-01,  5.2827e-01,  ..., -3.6590e-01,\n",
      "          -1.3925e-01, -7.8353e-01],\n",
      "         [ 1.4429e+00, -6.1786e-02, -1.7952e-01,  ...,  8.7200e-01,\n",
      "           8.4147e-01, -1.4253e+00],\n",
      "         [ 9.4006e-01, -7.5499e-01,  3.5424e-01,  ..., -7.0291e-01,\n",
      "           5.6228e-01,  7.0141e-01]]], grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[[-1.1066e+00,  1.3541e-01,  2.8545e-01,  ...,  6.1323e-01,\n",
      "           1.5040e-01,  8.6120e-01],\n",
      "         [ 1.5030e-01,  9.7659e-01, -5.2427e-01,  ..., -1.8798e-01,\n",
      "           2.1091e-01, -1.3520e-01],\n",
      "         [ 1.8113e-01,  1.7759e+00,  1.2599e+00,  ..., -6.6381e-01,\n",
      "           3.0515e-01, -9.4270e-01],\n",
      "         ...,\n",
      "         [ 3.7392e-02, -1.5967e-03, -6.1884e-01,  ..., -1.7571e+00,\n",
      "           1.7540e+00, -3.2654e-01],\n",
      "         [ 8.1910e-01, -1.5669e+00,  1.2085e+00,  ..., -1.1956e+00,\n",
      "           1.0348e+00,  1.3482e+00],\n",
      "         [ 7.5041e-01, -9.3876e-01,  3.3040e-01,  ..., -7.6609e-01,\n",
      "           7.1825e-01,  7.5741e-01]],\n",
      "\n",
      "        [[-8.5695e-01,  3.8930e-01,  2.6206e-01,  ...,  7.1772e-01,\n",
      "           1.9446e-02,  8.1998e-01],\n",
      "         [ 1.6079e-01,  1.2801e+00,  8.6234e-01,  ..., -3.6039e-01,\n",
      "          -9.2365e-01,  9.4000e-01],\n",
      "         [-4.8924e-02, -1.3295e+00, -1.9110e-01,  ...,  3.2695e-01,\n",
      "           8.6042e-01, -1.6152e+00],\n",
      "         ...,\n",
      "         [ 1.6257e+00,  9.6263e-01,  5.2827e-01,  ..., -3.6590e-01,\n",
      "          -1.3925e-01, -7.8353e-01],\n",
      "         [ 1.4429e+00, -6.1786e-02, -1.7952e-01,  ...,  8.7200e-01,\n",
      "           8.4147e-01, -1.4253e+00],\n",
      "         [ 9.4006e-01, -7.5499e-01,  3.5424e-01,  ..., -7.0291e-01,\n",
      "           5.6228e-01,  7.0141e-01]]], grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[[[-1.1066e+00,  1.3541e-01,  2.8545e-01,  ..., -1.4282e+00,\n",
      "            5.7845e-01,  1.9869e+00],\n",
      "          [-9.3723e-01,  2.1433e+00, -6.0527e-01,  ...,  6.1323e-01,\n",
      "            1.5040e-01,  8.6120e-01]],\n",
      "\n",
      "         [[ 1.5030e-01,  9.7659e-01, -5.2427e-01,  ..., -5.4982e-01,\n",
      "            1.4590e+00, -6.3993e-01],\n",
      "          [-4.8166e-01,  2.8023e+00, -7.2638e-01,  ..., -1.8798e-01,\n",
      "            2.1091e-01, -1.3520e-01]],\n",
      "\n",
      "         [[ 1.8113e-01,  1.7759e+00,  1.2599e+00,  ..., -1.3490e+00,\n",
      "           -4.3631e-01, -1.9226e-01],\n",
      "          [ 6.3409e-01,  8.5608e-01, -1.6727e-01,  ..., -6.6381e-01,\n",
      "            3.0515e-01, -9.4270e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.7392e-02, -1.5967e-03, -6.1884e-01,  ...,  7.9455e-02,\n",
      "            5.2953e-01, -7.4264e-01],\n",
      "          [-1.3475e+00,  1.4440e+00, -4.9198e-01,  ..., -1.7571e+00,\n",
      "            1.7540e+00, -3.2654e-01]],\n",
      "\n",
      "         [[ 8.1910e-01, -1.5669e+00,  1.2085e+00,  ..., -7.5222e-01,\n",
      "           -2.4625e-01,  9.0788e-02],\n",
      "          [-1.9644e+00,  4.0846e-02, -5.2588e-01,  ..., -1.1956e+00,\n",
      "            1.0348e+00,  1.3482e+00]],\n",
      "\n",
      "         [[ 7.5041e-01, -9.3876e-01,  3.3040e-01,  ...,  6.6582e-01,\n",
      "           -5.6189e-01,  2.5012e+00],\n",
      "          [-1.8658e+00,  1.5835e+00, -6.3080e-01,  ..., -7.6609e-01,\n",
      "            7.1825e-01,  7.5741e-01]]],\n",
      "\n",
      "\n",
      "        [[[-8.5695e-01,  3.8930e-01,  2.6206e-01,  ..., -1.3148e+00,\n",
      "            6.1240e-01,  1.9268e+00],\n",
      "          [-7.5582e-01,  1.9591e+00, -5.8878e-01,  ...,  7.1772e-01,\n",
      "            1.9446e-02,  8.1998e-01]],\n",
      "\n",
      "         [[ 1.6079e-01,  1.2801e+00,  8.6234e-01,  ..., -1.6560e-01,\n",
      "            8.5838e-01,  1.1964e+00],\n",
      "          [-9.4561e-01,  1.3090e+00,  1.1152e+00,  ..., -3.6039e-01,\n",
      "           -9.2365e-01,  9.4000e-01]],\n",
      "\n",
      "         [[-4.8924e-02, -1.3295e+00, -1.9110e-01,  ...,  9.3531e-02,\n",
      "            8.4504e-01,  5.3788e-02],\n",
      "          [ 1.5195e+00,  6.7555e-01, -8.5990e-01,  ...,  3.2695e-01,\n",
      "            8.6042e-01, -1.6152e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.6257e+00,  9.6263e-01,  5.2827e-01,  ..., -4.3126e-01,\n",
      "            6.1209e-01, -1.5583e+00],\n",
      "          [-3.7793e-01,  2.2847e-01, -4.4633e-01,  ..., -3.6590e-01,\n",
      "           -1.3925e-01, -7.8353e-01]],\n",
      "\n",
      "         [[ 1.4429e+00, -6.1786e-02, -1.7952e-01,  ..., -1.8817e+00,\n",
      "           -2.6757e-01,  5.6028e-01],\n",
      "          [-1.5686e+00,  2.5505e-01, -2.4039e-01,  ...,  8.7200e-01,\n",
      "            8.4147e-01, -1.4253e+00]],\n",
      "\n",
      "         [[ 9.4006e-01, -7.5499e-01,  3.5424e-01,  ...,  7.2596e-01,\n",
      "           -4.3662e-01,  2.5606e+00],\n",
      "          [-1.7819e+00,  1.4478e+00, -6.5401e-01,  ..., -7.0291e-01,\n",
      "            5.6228e-01,  7.0141e-01]]]], grad_fn=<ViewBackward0>)\n",
      "tensor([[[ 1.6618e-01, -7.6537e-02,  4.1417e-01,  ..., -1.7422e-02,\n",
      "          -2.0055e-01,  1.6219e-01],\n",
      "         [ 5.6780e-02, -7.0459e-01,  1.9737e-01,  ...,  3.5730e-01,\n",
      "          -4.4192e-01,  9.6264e-01],\n",
      "         [ 3.1790e-01,  2.2389e-01,  7.3157e-01,  ...,  6.3811e-02,\n",
      "          -6.7484e-02,  7.7601e-01],\n",
      "         ...,\n",
      "         [-1.2464e-01, -3.4039e-01,  1.6902e-01,  ...,  4.3149e-01,\n",
      "           2.7869e-01,  1.1378e-01],\n",
      "         [-3.9426e-01, -3.1928e-02, -3.0241e-01,  ...,  1.3737e-01,\n",
      "          -4.0578e-01,  3.2127e-01],\n",
      "         [ 1.6920e-01, -6.6848e-01,  1.7639e-01,  ..., -5.4677e-01,\n",
      "           3.6189e-01,  6.7341e-01]],\n",
      "\n",
      "        [[ 1.5138e-01,  1.2596e-03,  3.2680e-01,  ...,  3.2622e-03,\n",
      "          -2.7697e-01,  1.2571e-01],\n",
      "         [-1.4868e-01, -4.0971e-01, -1.9524e-01,  ..., -1.6030e-01,\n",
      "          -1.1902e+00,  6.4736e-01],\n",
      "         [ 3.8226e-01, -4.3612e-01,  9.2326e-01,  ...,  1.5999e-01,\n",
      "           2.8447e-01,  3.2073e-01],\n",
      "         ...,\n",
      "         [-3.6068e-01, -4.9994e-01,  6.2697e-04,  ...,  1.5768e-01,\n",
      "           1.5432e-01,  4.0530e-02],\n",
      "         [-2.5232e-02, -5.7773e-02, -4.3491e-01,  ...,  3.7023e-01,\n",
      "           7.9352e-02,  2.6874e-01],\n",
      "         [ 1.5005e-01, -6.2088e-01,  1.0767e-01,  ..., -5.1999e-01,\n",
      "           2.8638e-01,  6.5455e-01]]], grad_fn=<ViewBackward0>)\n",
      "tensor([[[ 1.6618e-01, -7.6537e-02,  4.1417e-01,  ..., -1.7422e-02,\n",
      "          -2.0055e-01,  1.6219e-01],\n",
      "         [ 5.6780e-02, -7.0459e-01,  1.9737e-01,  ...,  3.5730e-01,\n",
      "          -4.4192e-01,  9.6264e-01],\n",
      "         [ 3.1790e-01,  2.2389e-01,  7.3157e-01,  ...,  6.3811e-02,\n",
      "          -6.7484e-02,  7.7601e-01],\n",
      "         ...,\n",
      "         [-1.2464e-01, -3.4039e-01,  1.6902e-01,  ...,  4.3149e-01,\n",
      "           2.7869e-01,  1.1378e-01],\n",
      "         [-3.9426e-01, -3.1928e-02, -3.0241e-01,  ...,  1.3737e-01,\n",
      "          -4.0578e-01,  3.2127e-01],\n",
      "         [ 1.6920e-01, -6.6848e-01,  1.7639e-01,  ..., -5.4677e-01,\n",
      "           3.6189e-01,  6.7341e-01]],\n",
      "\n",
      "        [[ 1.5138e-01,  1.2596e-03,  3.2680e-01,  ...,  3.2622e-03,\n",
      "          -2.7697e-01,  1.2571e-01],\n",
      "         [-1.4868e-01, -4.0971e-01, -1.9524e-01,  ..., -1.6030e-01,\n",
      "          -1.1902e+00,  6.4736e-01],\n",
      "         [ 3.8226e-01, -4.3612e-01,  9.2326e-01,  ...,  1.5999e-01,\n",
      "           2.8447e-01,  3.2073e-01],\n",
      "         ...,\n",
      "         [-3.6068e-01, -4.9994e-01,  6.2697e-04,  ...,  1.5768e-01,\n",
      "           1.5432e-01,  4.0530e-02],\n",
      "         [-2.5232e-02, -5.7773e-02, -4.3491e-01,  ...,  3.7023e-01,\n",
      "           7.9352e-02,  2.6874e-01],\n",
      "         [ 1.5005e-01, -6.2088e-01,  1.0767e-01,  ..., -5.1999e-01,\n",
      "           2.8638e-01,  6.5455e-01]]], grad_fn=<ViewBackward0>)\n",
      "tensor([[[[ 1.6618e-01, -7.6537e-02,  4.1417e-01,  ...,  1.6215e-01,\n",
      "           -1.8829e-01, -2.8915e-01],\n",
      "          [ 5.5585e-01,  2.0756e-01, -3.0470e-01,  ..., -1.7422e-02,\n",
      "           -2.0055e-01,  1.6219e-01]],\n",
      "\n",
      "         [[ 5.6780e-02, -7.0459e-01,  1.9737e-01,  ..., -7.0225e-01,\n",
      "            8.9242e-02,  2.8796e-01],\n",
      "          [ 3.4328e-01,  2.4870e-01,  2.7090e-02,  ...,  3.5730e-01,\n",
      "           -4.4192e-01,  9.6264e-01]],\n",
      "\n",
      "         [[ 3.1790e-01,  2.2389e-01,  7.3157e-01,  ...,  3.5930e-01,\n",
      "            2.1712e-01,  2.2082e-01],\n",
      "          [ 4.8998e-01,  1.6628e-01, -1.3752e-01,  ...,  6.3811e-02,\n",
      "           -6.7484e-02,  7.7601e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.2464e-01, -3.4039e-01,  1.6902e-01,  ...,  2.8417e-01,\n",
      "           -1.5605e-01,  5.0318e-02],\n",
      "          [ 1.0306e+00, -3.1330e-01, -4.9133e-01,  ...,  4.3149e-01,\n",
      "            2.7869e-01,  1.1378e-01]],\n",
      "\n",
      "         [[-3.9426e-01, -3.1928e-02, -3.0241e-01,  ..., -1.1750e-01,\n",
      "           -2.7362e-01, -3.1321e-01],\n",
      "          [ 3.9945e-01, -2.5291e-01, -1.9467e-01,  ...,  1.3737e-01,\n",
      "           -4.0578e-01,  3.2127e-01]],\n",
      "\n",
      "         [[ 1.6920e-01, -6.6848e-01,  1.7639e-01,  ...,  6.5684e-01,\n",
      "            1.6376e-01, -1.1939e-01],\n",
      "          [ 2.8690e-01,  7.0239e-02, -2.0735e-01,  ..., -5.4677e-01,\n",
      "            3.6189e-01,  6.7341e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 1.5138e-01,  1.2596e-03,  3.2680e-01,  ...,  1.0565e-01,\n",
      "           -2.4117e-01, -3.0002e-01],\n",
      "          [ 4.6713e-01,  2.2525e-01, -2.5983e-01,  ...,  3.2622e-03,\n",
      "           -2.7697e-01,  1.2571e-01]],\n",
      "\n",
      "         [[-1.4868e-01, -4.0971e-01, -1.9524e-01,  ..., -2.7856e-01,\n",
      "           -1.5136e-01, -4.0815e-01],\n",
      "          [ 5.3124e-01, -3.8414e-01, -1.0627e-01,  ..., -1.6030e-01,\n",
      "           -1.1902e+00,  6.4736e-01]],\n",
      "\n",
      "         [[ 3.8226e-01, -4.3612e-01,  9.2326e-01,  ...,  2.5693e-01,\n",
      "            3.2857e-01,  6.0924e-01],\n",
      "          [ 1.2134e-01, -1.6585e-01,  3.0616e-01,  ...,  1.5999e-01,\n",
      "            2.8447e-01,  3.2073e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.6068e-01, -4.9994e-01,  6.2697e-04,  ...,  5.4880e-01,\n",
      "           -3.0533e-01, -4.6043e-01],\n",
      "          [ 2.7945e-01, -4.4638e-01,  3.6511e-02,  ...,  1.5768e-01,\n",
      "            1.5432e-01,  4.0530e-02]],\n",
      "\n",
      "         [[-2.5232e-02, -5.7773e-02, -4.3491e-01,  ...,  1.6575e-01,\n",
      "           -2.6052e-01, -1.4978e-01],\n",
      "          [ 3.0399e-01, -2.0002e-01,  5.3940e-02,  ...,  3.7023e-01,\n",
      "            7.9352e-02,  2.6874e-01]],\n",
      "\n",
      "         [[ 1.5005e-01, -6.2088e-01,  1.0767e-01,  ...,  5.8612e-01,\n",
      "            8.6824e-02, -9.4436e-02],\n",
      "          [ 2.1307e-01,  1.1317e-01, -1.1514e-01,  ..., -5.1999e-01,\n",
      "            2.8638e-01,  6.5455e-01]]]], grad_fn=<ViewBackward0>)\n",
      "tensor([[[-1.1066e+00,  1.3541e-01,  2.8545e-01,  ...,  6.1323e-01,\n",
      "           1.5040e-01,  8.6120e-01],\n",
      "         [ 1.5030e-01,  9.7659e-01, -5.2427e-01,  ..., -1.8798e-01,\n",
      "           2.1091e-01, -1.3520e-01],\n",
      "         [ 1.8113e-01,  1.7759e+00,  1.2599e+00,  ..., -6.6381e-01,\n",
      "           3.0515e-01, -9.4270e-01],\n",
      "         ...,\n",
      "         [ 3.7392e-02, -1.5967e-03, -6.1884e-01,  ..., -1.7571e+00,\n",
      "           1.7540e+00, -3.2654e-01],\n",
      "         [ 8.1910e-01, -1.5669e+00,  1.2085e+00,  ..., -1.1956e+00,\n",
      "           1.0348e+00,  1.3482e+00],\n",
      "         [ 7.5041e-01, -9.3876e-01,  3.3040e-01,  ..., -7.6609e-01,\n",
      "           7.1825e-01,  7.5741e-01]],\n",
      "\n",
      "        [[-8.5695e-01,  3.8930e-01,  2.6206e-01,  ...,  7.1772e-01,\n",
      "           1.9446e-02,  8.1998e-01],\n",
      "         [ 1.6079e-01,  1.2801e+00,  8.6234e-01,  ..., -3.6039e-01,\n",
      "          -9.2365e-01,  9.4000e-01],\n",
      "         [-4.8924e-02, -1.3295e+00, -1.9110e-01,  ...,  3.2695e-01,\n",
      "           8.6042e-01, -1.6152e+00],\n",
      "         ...,\n",
      "         [ 1.6257e+00,  9.6263e-01,  5.2827e-01,  ..., -3.6590e-01,\n",
      "          -1.3925e-01, -7.8353e-01],\n",
      "         [ 1.4429e+00, -6.1786e-02, -1.7952e-01,  ...,  8.7200e-01,\n",
      "           8.4147e-01, -1.4253e+00],\n",
      "         [ 9.4006e-01, -7.5499e-01,  3.5424e-01,  ..., -7.0291e-01,\n",
      "           5.6228e-01,  7.0141e-01]]], grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[[-1.1066e+00,  1.3541e-01,  2.8545e-01,  ...,  6.1323e-01,\n",
      "           1.5040e-01,  8.6120e-01],\n",
      "         [ 1.5030e-01,  9.7659e-01, -5.2427e-01,  ..., -1.8798e-01,\n",
      "           2.1091e-01, -1.3520e-01],\n",
      "         [ 1.8113e-01,  1.7759e+00,  1.2599e+00,  ..., -6.6381e-01,\n",
      "           3.0515e-01, -9.4270e-01],\n",
      "         ...,\n",
      "         [ 3.7392e-02, -1.5967e-03, -6.1884e-01,  ..., -1.7571e+00,\n",
      "           1.7540e+00, -3.2654e-01],\n",
      "         [ 8.1910e-01, -1.5669e+00,  1.2085e+00,  ..., -1.1956e+00,\n",
      "           1.0348e+00,  1.3482e+00],\n",
      "         [ 7.5041e-01, -9.3876e-01,  3.3040e-01,  ..., -7.6609e-01,\n",
      "           7.1825e-01,  7.5741e-01]],\n",
      "\n",
      "        [[-8.5695e-01,  3.8930e-01,  2.6206e-01,  ...,  7.1772e-01,\n",
      "           1.9446e-02,  8.1998e-01],\n",
      "         [ 1.6079e-01,  1.2801e+00,  8.6234e-01,  ..., -3.6039e-01,\n",
      "          -9.2365e-01,  9.4000e-01],\n",
      "         [-4.8924e-02, -1.3295e+00, -1.9110e-01,  ...,  3.2695e-01,\n",
      "           8.6042e-01, -1.6152e+00],\n",
      "         ...,\n",
      "         [ 1.6257e+00,  9.6263e-01,  5.2827e-01,  ..., -3.6590e-01,\n",
      "          -1.3925e-01, -7.8353e-01],\n",
      "         [ 1.4429e+00, -6.1786e-02, -1.7952e-01,  ...,  8.7200e-01,\n",
      "           8.4147e-01, -1.4253e+00],\n",
      "         [ 9.4006e-01, -7.5499e-01,  3.5424e-01,  ..., -7.0291e-01,\n",
      "           5.6228e-01,  7.0141e-01]]], grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[[[-1.1066e+00,  1.3541e-01,  2.8545e-01,  ..., -1.4282e+00,\n",
      "            5.7845e-01,  1.9869e+00],\n",
      "          [-9.3723e-01,  2.1433e+00, -6.0527e-01,  ...,  6.1323e-01,\n",
      "            1.5040e-01,  8.6120e-01]],\n",
      "\n",
      "         [[ 1.5030e-01,  9.7659e-01, -5.2427e-01,  ..., -5.4982e-01,\n",
      "            1.4590e+00, -6.3993e-01],\n",
      "          [-4.8166e-01,  2.8023e+00, -7.2638e-01,  ..., -1.8798e-01,\n",
      "            2.1091e-01, -1.3520e-01]],\n",
      "\n",
      "         [[ 1.8113e-01,  1.7759e+00,  1.2599e+00,  ..., -1.3490e+00,\n",
      "           -4.3631e-01, -1.9226e-01],\n",
      "          [ 6.3409e-01,  8.5608e-01, -1.6727e-01,  ..., -6.6381e-01,\n",
      "            3.0515e-01, -9.4270e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.7392e-02, -1.5967e-03, -6.1884e-01,  ...,  7.9455e-02,\n",
      "            5.2953e-01, -7.4264e-01],\n",
      "          [-1.3475e+00,  1.4440e+00, -4.9198e-01,  ..., -1.7571e+00,\n",
      "            1.7540e+00, -3.2654e-01]],\n",
      "\n",
      "         [[ 8.1910e-01, -1.5669e+00,  1.2085e+00,  ..., -7.5222e-01,\n",
      "           -2.4625e-01,  9.0788e-02],\n",
      "          [-1.9644e+00,  4.0846e-02, -5.2588e-01,  ..., -1.1956e+00,\n",
      "            1.0348e+00,  1.3482e+00]],\n",
      "\n",
      "         [[ 7.5041e-01, -9.3876e-01,  3.3040e-01,  ...,  6.6582e-01,\n",
      "           -5.6189e-01,  2.5012e+00],\n",
      "          [-1.8658e+00,  1.5835e+00, -6.3080e-01,  ..., -7.6609e-01,\n",
      "            7.1825e-01,  7.5741e-01]]],\n",
      "\n",
      "\n",
      "        [[[-8.5695e-01,  3.8930e-01,  2.6206e-01,  ..., -1.3148e+00,\n",
      "            6.1240e-01,  1.9268e+00],\n",
      "          [-7.5582e-01,  1.9591e+00, -5.8878e-01,  ...,  7.1772e-01,\n",
      "            1.9446e-02,  8.1998e-01]],\n",
      "\n",
      "         [[ 1.6079e-01,  1.2801e+00,  8.6234e-01,  ..., -1.6560e-01,\n",
      "            8.5838e-01,  1.1964e+00],\n",
      "          [-9.4561e-01,  1.3090e+00,  1.1152e+00,  ..., -3.6039e-01,\n",
      "           -9.2365e-01,  9.4000e-01]],\n",
      "\n",
      "         [[-4.8924e-02, -1.3295e+00, -1.9110e-01,  ...,  9.3531e-02,\n",
      "            8.4504e-01,  5.3788e-02],\n",
      "          [ 1.5195e+00,  6.7555e-01, -8.5990e-01,  ...,  3.2695e-01,\n",
      "            8.6042e-01, -1.6152e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.6257e+00,  9.6263e-01,  5.2827e-01,  ..., -4.3126e-01,\n",
      "            6.1209e-01, -1.5583e+00],\n",
      "          [-3.7793e-01,  2.2847e-01, -4.4633e-01,  ..., -3.6590e-01,\n",
      "           -1.3925e-01, -7.8353e-01]],\n",
      "\n",
      "         [[ 1.4429e+00, -6.1786e-02, -1.7952e-01,  ..., -1.8817e+00,\n",
      "           -2.6757e-01,  5.6028e-01],\n",
      "          [-1.5686e+00,  2.5505e-01, -2.4039e-01,  ...,  8.7200e-01,\n",
      "            8.4147e-01, -1.4253e+00]],\n",
      "\n",
      "         [[ 9.4006e-01, -7.5499e-01,  3.5424e-01,  ...,  7.2596e-01,\n",
      "           -4.3662e-01,  2.5606e+00],\n",
      "          [-1.7819e+00,  1.4478e+00, -6.5401e-01,  ..., -7.0291e-01,\n",
      "            5.6228e-01,  7.0141e-01]]]], grad_fn=<ViewBackward0>)\n",
      "tensor([[[[ 0.3532,  0.3525,  0.8094,  ..., -0.7191,  0.0268, -0.1268],\n",
      "          [ 0.1370,  0.0349,  0.3261,  ..., -0.5433,  0.1826,  0.1389],\n",
      "          [ 0.2985, -0.1241,  0.2607,  ..., -0.4944,  0.2074, -0.1919],\n",
      "          ...,\n",
      "          [ 0.1769,  0.0558,  0.1655,  ..., -0.5958,  0.0589,  0.1595],\n",
      "          [ 0.1170, -0.0773,  0.2948,  ..., -0.4506,  0.2259,  0.0738],\n",
      "          [ 0.2496, -0.0622,  0.3210,  ..., -0.3936,  0.1540,  0.2698]],\n",
      "\n",
      "         [[-0.3160,  1.2153, -0.5020,  ..., -0.8121,  0.1875, -0.5366],\n",
      "          [-0.7255,  1.4399, -0.3987,  ..., -0.6742,  0.1828, -0.0927],\n",
      "          [-0.7015,  1.2927, -0.3470,  ..., -0.8568,  0.3436, -0.1047],\n",
      "          ...,\n",
      "          [-0.5971,  1.3925, -0.4098,  ..., -0.7403,  0.1817, -0.2435],\n",
      "          [-0.7247,  1.4003, -0.4456,  ..., -0.7375,  0.2319, -0.1372],\n",
      "          [-0.6677,  1.5476, -0.4333,  ..., -0.6766,  0.2331, -0.1934]]],\n",
      "\n",
      "\n",
      "        [[[ 0.7660, -0.4033, -0.0865,  ..., -0.7369,  0.1114,  0.7087],\n",
      "          [ 0.3852,  0.0557,  0.3837,  ..., -0.4184,  0.3301,  0.3412],\n",
      "          [ 0.5204, -0.0389,  0.2897,  ..., -0.3734,  0.1870,  0.6167],\n",
      "          ...,\n",
      "          [ 0.4538, -0.1297,  0.2719,  ..., -0.4525,  0.2538,  0.3691],\n",
      "          [ 0.3690,  0.1776,  0.4413,  ..., -0.3489,  0.3062,  0.4474],\n",
      "          [ 0.4862, -0.0651,  0.3087,  ..., -0.3055,  0.2125,  0.8039]],\n",
      "\n",
      "         [[-0.3557,  0.7324, -0.2931,  ..., -0.6549,  1.3186, -1.3545],\n",
      "          [-0.4426,  1.2427, -0.0900,  ..., -0.3629,  0.4969, -0.4142],\n",
      "          [-0.6201,  1.2541, -0.0024,  ..., -0.2662,  0.3647, -0.3462],\n",
      "          ...,\n",
      "          [-0.7380,  1.0923,  0.0526,  ..., -0.1179,  0.3767, -0.5448],\n",
      "          [-0.8032,  1.0826,  0.0519,  ..., -0.1864,  0.4372, -0.5445],\n",
      "          [-0.4491,  1.0590, -0.0709,  ..., -0.1897,  0.2580, -0.4726]]]],\n",
      "       grad_fn=<ScaledDotProductFlashAttentionForCpuBackward0>)\n",
      "tensor([[[[ 0.3532,  0.3525,  0.8094,  ..., -0.7191,  0.0268, -0.1268],\n",
      "          [-0.3160,  1.2153, -0.5020,  ..., -0.8121,  0.1875, -0.5366]],\n",
      "\n",
      "         [[ 0.1370,  0.0349,  0.3261,  ..., -0.5433,  0.1826,  0.1389],\n",
      "          [-0.7255,  1.4399, -0.3987,  ..., -0.6742,  0.1828, -0.0927]],\n",
      "\n",
      "         [[ 0.2985, -0.1241,  0.2607,  ..., -0.4944,  0.2074, -0.1919],\n",
      "          [-0.7015,  1.2927, -0.3470,  ..., -0.8568,  0.3436, -0.1047]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.1769,  0.0558,  0.1655,  ..., -0.5958,  0.0589,  0.1595],\n",
      "          [-0.5971,  1.3925, -0.4098,  ..., -0.7403,  0.1817, -0.2435]],\n",
      "\n",
      "         [[ 0.1170, -0.0773,  0.2948,  ..., -0.4506,  0.2259,  0.0738],\n",
      "          [-0.7247,  1.4003, -0.4456,  ..., -0.7375,  0.2319, -0.1372]],\n",
      "\n",
      "         [[ 0.2496, -0.0622,  0.3210,  ..., -0.3936,  0.1540,  0.2698],\n",
      "          [-0.6677,  1.5476, -0.4333,  ..., -0.6766,  0.2331, -0.1934]]],\n",
      "\n",
      "\n",
      "        [[[ 0.7660, -0.4033, -0.0865,  ..., -0.7369,  0.1114,  0.7087],\n",
      "          [-0.3557,  0.7324, -0.2931,  ..., -0.6549,  1.3186, -1.3545]],\n",
      "\n",
      "         [[ 0.3852,  0.0557,  0.3837,  ..., -0.4184,  0.3301,  0.3412],\n",
      "          [-0.4426,  1.2427, -0.0900,  ..., -0.3629,  0.4969, -0.4142]],\n",
      "\n",
      "         [[ 0.5204, -0.0389,  0.2897,  ..., -0.3734,  0.1870,  0.6167],\n",
      "          [-0.6201,  1.2541, -0.0024,  ..., -0.2662,  0.3647, -0.3462]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.4538, -0.1297,  0.2719,  ..., -0.4525,  0.2538,  0.3691],\n",
      "          [-0.7380,  1.0923,  0.0526,  ..., -0.1179,  0.3767, -0.5448]],\n",
      "\n",
      "         [[ 0.3690,  0.1776,  0.4413,  ..., -0.3489,  0.3062,  0.4474],\n",
      "          [-0.8032,  1.0826,  0.0519,  ..., -0.1864,  0.4372, -0.5445]],\n",
      "\n",
      "         [[ 0.4862, -0.0651,  0.3087,  ..., -0.3055,  0.2125,  0.8039],\n",
      "          [-0.4491,  1.0590, -0.0709,  ..., -0.1897,  0.2580, -0.4726]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mPruning module: bert_encoder_layer_0_attention_self_query\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPruning module: bert_encoder_layer_0_attention_self_key\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPruning module: bert_encoder_layer_0_attention_self_value\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPruning module: bert_encoder_layer_0_intermediate_dense\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPruning module: bert_encoder_layer_0_output_dense\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPruning module: bert_encoder_layer_1_attention_self_key\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPruning module: bert_encoder_layer_1_intermediate_dense\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPruning module: bert_encoder_layer_1_output_dense\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPruning module: classifier\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from chop.pipelines import CompressionPipeline\n",
    "from chop import MaseGraph\n",
    "\n",
    "mg = MaseGraph(model)\n",
    "pipe = CompressionPipeline()\n",
    "\n",
    "quantization_config = {\n",
    "    \"by\": \"type\",\n",
    "    \"default\": {\n",
    "        \"config\": {\n",
    "            \"name\": None,\n",
    "        }\n",
    "    },\n",
    "    \"linear\": {\n",
    "        \"config\": {\n",
    "            \"name\": \"integer\",\n",
    "            # data\n",
    "            \"data_in_width\": 8,\n",
    "            \"data_in_frac_width\": 4,\n",
    "            # weight\n",
    "            \"weight_width\": 8,\n",
    "            \"weight_frac_width\": 4,\n",
    "            # bias\n",
    "            \"bias_width\": 8,\n",
    "            \"bias_frac_width\": 4,\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "pruning_config = {\n",
    "    \"weight\": {\n",
    "        \"sparsity\": 0.5,\n",
    "        \"method\": \"l1-norm\",\n",
    "        \"scope\": \"local\",\n",
    "    },\n",
    "    \"activation\": {\n",
    "        \"sparsity\": 0.5,\n",
    "        \"method\": \"l1-norm\",\n",
    "        \"scope\": \"local\",\n",
    "    },\n",
    "}\n",
    "\n",
    "mg, _ = pipe(\n",
    "    mg,\n",
    "    pass_args={\n",
    "        \"quantize_transform_pass\": quantization_config,\n",
    "        \"prune_transform_pass\": pruning_config,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from optuna import Study\n",
    "from optuna.trial import FrozenTrial\n",
    "\n",
    "def samplerTrial(sampler_name:str, trials:int, sampler, f=objective) -> None:\n",
    "\n",
    "    data = [0]*trials\n",
    "\n",
    "    def record_accuracy_callback(stud:Study, fzt:FrozenTrial):\n",
    "        print(f\"Trial: {fzt.number}, Accuracy: {fzt.value}\")\n",
    "        data[fzt.number-1] ={\"n\":fzt.number, \"accuracy\":fzt.value}\n",
    "\n",
    "    if (sampler == None):\n",
    "        raise RuntimeError(\"No Sampler Provided\")\n",
    "\n",
    "    study = optuna.create_study(\n",
    "        direction=\"maximize\",\n",
    "        study_name=f\"bert-tiny-nas-{sampler_name}-study\",\n",
    "        sampler=sampler\n",
    "    )\n",
    "\n",
    "    study.optimize(\n",
    "        f,\n",
    "        n_trials=trials,\n",
    "        timeout=60*60*24,\n",
    "        callbacks=[record_accuracy_callback]\n",
    "    )\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(f\"{Path.home()}/mase/tasks/tutorial5/sampler_run_{sampler_name}.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samplerTrial(sampler_name=\"grid\", trials=100, sampler=sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "df_r = pd.read_csv(f\"{Path.home()}/mase/tasks/tutorial5/random.csv\")\n",
    "df_t = pd.read_csv(f\"{Path.home()}/mase/tasks/tutorial5/tpes.csv\")\n",
    "df_g = pd.read_csv(f\"{Path.home()}/mase/tasks/tutorial5/grid.csv\")\n",
    "fig_pretrain = plt.figure()\n",
    "\n",
    "plt.scatter(x=df_r['n'], y=df_r['accuracy'] * 100, marker='x', s=20)\n",
    "plt.scatter(x=df_t['n'], y=df_t['accuracy'] * 100, marker='x', s=20)\n",
    "plt.scatter(x=df_g['n'], y=df_g['accuracy'] * 100, marker='x', s=20)\n",
    "plt.axhline(y=baseline * 100, linestyle='--', linewidth=0.8)\n",
    "plt.legend(['Random Sampler', 'TPES Sampler', 'Grid Sampler', 'Baseline'], bbox_to_anchor=(1.04, 1), loc=\"upper left\")\n",
    "plt.xlabel(\"Trials\")\n",
    "plt.ylabel(\"Accuracy %\")\n",
    "plt.title(\"Trials vs Accuracy\")\n",
    "plt.savefig(f\"{Path.home()}/mase/tasks/tutorial5/samplers\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model):\n",
    "\n",
    "    trainer = get_trainer(\n",
    "        model = model,\n",
    "        tokenized_dataset = dataset,\n",
    "        tokenizer = tokenizer,\n",
    "        evaluate_metric = \"accuracy\",\n",
    "        num_train_epochs = 1\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    eval_results = trainer.evaluate()\n",
    "\n",
    "    return eval_results[\"eval_accuracy\"]\n",
    "\n",
    "\n",
    "def pipeObjective(trial):\n",
    "    model = construct_model(trial)\n",
    "\n",
    "    post_t1 = train(model)\n",
    "    print(f'Post Train: {post_t1}')\n",
    "\n",
    "    model.cpu()\n",
    "\n",
    "    mg = MaseGraph(model=model)\n",
    "\n",
    "    qc = quantization_config\n",
    "    pc = pruning_config\n",
    "\n",
    "    mg, _ = pipe(\n",
    "            mg,\n",
    "            pass_args={\n",
    "                \"quantize_transform_pass\": qc,\n",
    "                \"prune_transform_pass\": pc,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    trainer = get_trainer(\n",
    "        model = model,\n",
    "        tokenized_dataset = dataset,\n",
    "        tokenizer = tokenizer,\n",
    "        evaluate_metric = \"accuracy\",\n",
    "        num_train_epochs = 1\n",
    "    )\n",
    "\n",
    "    post_comp_no_t = trainer.evaluate()\n",
    "\n",
    "    print(f'Post Compressing: {post_comp_no_t}')\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    eval_results = trainer.evaluate()\n",
    "\n",
    "    print(f'Final: {eval_results[\"eval_accuracy\"]}')\n",
    "\n",
    "    trial.set_user_attr(\"model\", mg.model)\n",
    "\n",
    "    return eval_results[\"eval_accuracy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-02 23:10:21,514] A new study created in memory with name: bert-tiny-nas-study\n",
      "/rds/general/user/oa321/home/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "WARNING:accelerate.utils.other:Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:49, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.688300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.561600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.483500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.414800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.365300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.362200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:23]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`past_key_values` were not specified as input names, but model.config.use_cache = True. Setting model.config.use_cache = False.\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mGetting dummy input for prajjwal1/bert-tiny.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post Train: 0.85004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-02-02 23:11:36,530] Trial 0 failed with parameters: {'num_layers': 1, 'num_heads': 1, 'hidden_size': 2, 'intermediate_size': 2, 'bert.encoder.layer.0.attention.self.query_type': 'identity', 'bert.encoder.layer.0.attention.self.key_type': 'identity', 'bert.encoder.layer.0.attention.self.value_type': 'linear', 'bert.encoder.layer.0.attention.output.dense_type': 'identity', 'bert.encoder.layer.1.attention.self.query_type': 'linear', 'bert.encoder.layer.1.attention.self.key_type': 'linear', 'bert.encoder.layer.1.attention.self.value_type': 'identity', 'bert.encoder.layer.1.attention.output.dense_type': 'linear', 'bert.pooler.dense_type': 'linear'} because of the following error: KeyError('by').\n",
      "Traceback (most recent call last):\n",
      "  File \"/rds/general/user/oa321/home/miniforge3/envs/mase_env/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/var/tmp/pbs.566831.pbs/ipykernel_975000/1430720872.py\", line 31, in pipeObjective\n",
      "    mg, _ = pipe(\n",
      "            ^^^^^\n",
      "  File \"/rds/general/user/oa321/home/mase/src/chop/pipelines/auto_pipeline.py\", line 72, in __call__\n",
      "    mg, pass_outputs = self._run_pass_group(\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/rds/general/user/oa321/home/mase/src/chop/pipelines/auto_pipeline.py\", line 53, in _run_pass_group\n",
      "    mg, pass_output = pass_fn(mg, pass_args=args)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/rds/general/user/oa321/home/mase/src/chop/passes/graph/transforms/quantize/quantize.py\", line 248, in quantize_transform_pass\n",
      "    by = pass_args.pop(\"by\")\n",
      "         ^^^^^^^^^^^^^^^^^^^\n",
      "KeyError: 'by'\n",
      "[W 2025-02-02 23:11:36,538] Trial 0 failed with value None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101, 9932, 2089, 2202, 2058, 1996, 2088, 2028, 2154,  102],\n",
      "        [ 101, 2023, 2003, 2339, 2017, 2323, 4553, 4748, 4877,  102]])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tensor([[ 101, 9932, 2089, 2202, 2058, 1996, 2088, 2028, 2154,  102],\n",
      "        [ 101, 2023, 2003, 2339, 2017, 2323, 4553, 4748, 4877,  102]])\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "tensor([[[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]]])\n",
      "tensor([[[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]]])\n",
      "tensor([[[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]])\n",
      "tensor([[[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]])\n",
      "tensor([[[-0.3880, -0.9894,  0.9876,  ...,  0.0966, -0.4390,  0.2319],\n",
      "         [-0.6174, -0.2140,  1.7522,  ..., -0.3963,  0.6241,  0.6780],\n",
      "         [ 0.1341, -0.1296, -0.6382,  ..., -0.4098, -0.8227,  0.9676],\n",
      "         ...,\n",
      "         [ 0.1024,  0.8682, -0.2822,  ...,  0.8694, -0.5670,  0.0173],\n",
      "         [-0.4227, -0.1533, -0.3317,  ...,  0.7912, -0.7795,  1.5146],\n",
      "         [-0.0220,  1.9235, -0.3443,  ...,  0.6882, -2.7216,  1.4170]],\n",
      "\n",
      "        [[-0.3880, -0.9894,  0.9876,  ...,  0.0966, -0.4390,  0.2319],\n",
      "         [ 0.1792,  1.2004,  0.7599,  ..., -0.1489, -1.3360,  0.8866],\n",
      "         [ 0.0229, -0.4318, -0.8361,  ...,  0.1343, -1.1484, -0.6132],\n",
      "         ...,\n",
      "         [ 0.3785,  0.0089,  0.0674,  ...,  1.7756, -0.6305, -0.7159],\n",
      "         [ 0.4789,  0.8376, -0.2197,  ...,  1.7024, -0.9003, -0.1118],\n",
      "         [-0.0220,  1.9235, -0.3443,  ...,  0.6882, -2.7216,  1.4170]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[[-0.3880, -0.9894,  0.9876,  ...,  0.0966, -0.4390,  0.2319],\n",
      "         [-0.6174, -0.2140,  1.7522,  ..., -0.3963,  0.6241,  0.6780],\n",
      "         [ 0.1341, -0.1296, -0.6382,  ..., -0.4098, -0.8227,  0.9676],\n",
      "         ...,\n",
      "         [ 0.1024,  0.8682, -0.2822,  ...,  0.8694, -0.5670,  0.0173],\n",
      "         [-0.4227, -0.1533, -0.3317,  ...,  0.7912, -0.7795,  1.5146],\n",
      "         [-0.0220,  1.9235, -0.3443,  ...,  0.6882, -2.7216,  1.4170]],\n",
      "\n",
      "        [[-0.3880, -0.9894,  0.9876,  ...,  0.0966, -0.4390,  0.2319],\n",
      "         [ 0.1792,  1.2004,  0.7599,  ..., -0.1489, -1.3360,  0.8866],\n",
      "         [ 0.0229, -0.4318, -0.8361,  ...,  0.1343, -1.1484, -0.6132],\n",
      "         ...,\n",
      "         [ 0.3785,  0.0089,  0.0674,  ...,  1.7756, -0.6305, -0.7159],\n",
      "         [ 0.4789,  0.8376, -0.2197,  ...,  1.7024, -0.9003, -0.1118],\n",
      "         [-0.0220,  1.9235, -0.3443,  ...,  0.6882, -2.7216,  1.4170]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[[-0.3880, -0.9894,  0.9876,  ...,  0.0966, -0.4390,  0.2319],\n",
      "         [-0.6174, -0.2140,  1.7522,  ..., -0.3963,  0.6241,  0.6780],\n",
      "         [ 0.1341, -0.1296, -0.6382,  ..., -0.4098, -0.8227,  0.9676],\n",
      "         ...,\n",
      "         [ 0.1024,  0.8682, -0.2822,  ...,  0.8694, -0.5670,  0.0173],\n",
      "         [-0.4227, -0.1533, -0.3317,  ...,  0.7912, -0.7795,  1.5146],\n",
      "         [-0.0220,  1.9235, -0.3443,  ...,  0.6882, -2.7216,  1.4170]],\n",
      "\n",
      "        [[-0.3880, -0.9894,  0.9876,  ...,  0.0966, -0.4390,  0.2319],\n",
      "         [ 0.1792,  1.2004,  0.7599,  ..., -0.1489, -1.3360,  0.8866],\n",
      "         [ 0.0229, -0.4318, -0.8361,  ...,  0.1343, -1.1484, -0.6132],\n",
      "         ...,\n",
      "         [ 0.3785,  0.0089,  0.0674,  ...,  1.7756, -0.6305, -0.7159],\n",
      "         [ 0.4789,  0.8376, -0.2197,  ...,  1.7024, -0.9003, -0.1118],\n",
      "         [-0.0220,  1.9235, -0.3443,  ...,  0.6882, -2.7216,  1.4170]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[[[-0.3880, -0.9894,  0.9876,  ..., -0.4453,  1.7072,  1.3480],\n",
      "          [-0.1069,  1.5022,  1.0334,  ...,  0.0966, -0.4390,  0.2319]],\n",
      "\n",
      "         [[-0.6174, -0.2140,  1.7522,  ..., -0.0219,  0.4010, -0.3334],\n",
      "          [ 0.1341,  1.2262,  1.1812,  ..., -0.3963,  0.6241,  0.6780]],\n",
      "\n",
      "         [[ 0.1341, -0.1296, -0.6382,  ...,  0.5478, -0.8318, -0.3939],\n",
      "          [ 0.9034, -1.0218,  0.8145,  ..., -0.4098, -0.8227,  0.9676]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.1024,  0.8682, -0.2822,  ...,  0.8975,  0.4474,  1.8627],\n",
      "          [ 0.3347,  0.4947, -0.0716,  ...,  0.8694, -0.5670,  0.0173]],\n",
      "\n",
      "         [[-0.4227, -0.1533, -0.3317,  ...,  0.4884, -0.1049,  0.7318],\n",
      "          [ 1.3634,  2.0702, -1.4778,  ...,  0.7912, -0.7795,  1.5146]],\n",
      "\n",
      "         [[-0.0220,  1.9235, -0.3443,  ..., -0.3315,  2.1624,  0.9569],\n",
      "          [ 0.5618,  1.0137,  0.0877,  ...,  0.6882, -2.7216,  1.4170]]],\n",
      "\n",
      "\n",
      "        [[[-0.3880, -0.9894,  0.9876,  ..., -0.4453,  1.7072,  1.3480],\n",
      "          [-0.1069,  1.5022,  1.0334,  ...,  0.0966, -0.4390,  0.2319]],\n",
      "\n",
      "         [[ 0.1792,  1.2004,  0.7599,  ..., -0.8323, -0.4171,  1.0303],\n",
      "          [-0.2775,  0.4140,  0.1912,  ..., -0.1489, -1.3360,  0.8866]],\n",
      "\n",
      "         [[ 0.0229, -0.4318, -0.8361,  ...,  0.3474,  0.4603,  0.3874],\n",
      "          [ 1.3044, -0.0823,  0.4758,  ...,  0.1343, -1.1484, -0.6132]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.3785,  0.0089,  0.0674,  ..., -0.2682,  1.4370,  1.6999],\n",
      "          [-0.1082,  0.5477,  0.3929,  ...,  1.7756, -0.6305, -0.7159]],\n",
      "\n",
      "         [[ 0.4789,  0.8376, -0.2197,  ...,  1.1087,  2.3084,  0.2624],\n",
      "          [ 1.0364,  2.6073, -0.5881,  ...,  1.7024, -0.9003, -0.1118]],\n",
      "\n",
      "         [[-0.0220,  1.9235, -0.3443,  ..., -0.3315,  2.1624,  0.9569],\n",
      "          [ 0.5618,  1.0137,  0.0877,  ...,  0.6882, -2.7216,  1.4170]]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[-0.3880, -0.9894,  0.9876,  ...,  0.0966, -0.4390,  0.2319],\n",
      "         [-0.6174, -0.2140,  1.7522,  ..., -0.3963,  0.6241,  0.6780],\n",
      "         [ 0.1341, -0.1296, -0.6382,  ..., -0.4098, -0.8227,  0.9676],\n",
      "         ...,\n",
      "         [ 0.1024,  0.8682, -0.2822,  ...,  0.8694, -0.5670,  0.0173],\n",
      "         [-0.4227, -0.1533, -0.3317,  ...,  0.7912, -0.7795,  1.5146],\n",
      "         [-0.0220,  1.9235, -0.3443,  ...,  0.6882, -2.7216,  1.4170]],\n",
      "\n",
      "        [[-0.3880, -0.9894,  0.9876,  ...,  0.0966, -0.4390,  0.2319],\n",
      "         [ 0.1792,  1.2004,  0.7599,  ..., -0.1489, -1.3360,  0.8866],\n",
      "         [ 0.0229, -0.4318, -0.8361,  ...,  0.1343, -1.1484, -0.6132],\n",
      "         ...,\n",
      "         [ 0.3785,  0.0089,  0.0674,  ...,  1.7756, -0.6305, -0.7159],\n",
      "         [ 0.4789,  0.8376, -0.2197,  ...,  1.7024, -0.9003, -0.1118],\n",
      "         [-0.0220,  1.9235, -0.3443,  ...,  0.6882, -2.7216,  1.4170]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[[-0.3880, -0.9894,  0.9876,  ...,  0.0966, -0.4390,  0.2319],\n",
      "         [-0.6174, -0.2140,  1.7522,  ..., -0.3963,  0.6241,  0.6780],\n",
      "         [ 0.1341, -0.1296, -0.6382,  ..., -0.4098, -0.8227,  0.9676],\n",
      "         ...,\n",
      "         [ 0.1024,  0.8682, -0.2822,  ...,  0.8694, -0.5670,  0.0173],\n",
      "         [-0.4227, -0.1533, -0.3317,  ...,  0.7912, -0.7795,  1.5146],\n",
      "         [-0.0220,  1.9235, -0.3443,  ...,  0.6882, -2.7216,  1.4170]],\n",
      "\n",
      "        [[-0.3880, -0.9894,  0.9876,  ...,  0.0966, -0.4390,  0.2319],\n",
      "         [ 0.1792,  1.2004,  0.7599,  ..., -0.1489, -1.3360,  0.8866],\n",
      "         [ 0.0229, -0.4318, -0.8361,  ...,  0.1343, -1.1484, -0.6132],\n",
      "         ...,\n",
      "         [ 0.3785,  0.0089,  0.0674,  ...,  1.7756, -0.6305, -0.7159],\n",
      "         [ 0.4789,  0.8376, -0.2197,  ...,  1.7024, -0.9003, -0.1118],\n",
      "         [-0.0220,  1.9235, -0.3443,  ...,  0.6882, -2.7216,  1.4170]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[[[-0.3880, -0.9894,  0.9876,  ..., -0.4453,  1.7072,  1.3480],\n",
      "          [-0.1069,  1.5022,  1.0334,  ...,  0.0966, -0.4390,  0.2319]],\n",
      "\n",
      "         [[-0.6174, -0.2140,  1.7522,  ..., -0.0219,  0.4010, -0.3334],\n",
      "          [ 0.1341,  1.2262,  1.1812,  ..., -0.3963,  0.6241,  0.6780]],\n",
      "\n",
      "         [[ 0.1341, -0.1296, -0.6382,  ...,  0.5478, -0.8318, -0.3939],\n",
      "          [ 0.9034, -1.0218,  0.8145,  ..., -0.4098, -0.8227,  0.9676]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.1024,  0.8682, -0.2822,  ...,  0.8975,  0.4474,  1.8627],\n",
      "          [ 0.3347,  0.4947, -0.0716,  ...,  0.8694, -0.5670,  0.0173]],\n",
      "\n",
      "         [[-0.4227, -0.1533, -0.3317,  ...,  0.4884, -0.1049,  0.7318],\n",
      "          [ 1.3634,  2.0702, -1.4778,  ...,  0.7912, -0.7795,  1.5146]],\n",
      "\n",
      "         [[-0.0220,  1.9235, -0.3443,  ..., -0.3315,  2.1624,  0.9569],\n",
      "          [ 0.5618,  1.0137,  0.0877,  ...,  0.6882, -2.7216,  1.4170]]],\n",
      "\n",
      "\n",
      "        [[[-0.3880, -0.9894,  0.9876,  ..., -0.4453,  1.7072,  1.3480],\n",
      "          [-0.1069,  1.5022,  1.0334,  ...,  0.0966, -0.4390,  0.2319]],\n",
      "\n",
      "         [[ 0.1792,  1.2004,  0.7599,  ..., -0.8323, -0.4171,  1.0303],\n",
      "          [-0.2775,  0.4140,  0.1912,  ..., -0.1489, -1.3360,  0.8866]],\n",
      "\n",
      "         [[ 0.0229, -0.4318, -0.8361,  ...,  0.3474,  0.4603,  0.3874],\n",
      "          [ 1.3044, -0.0823,  0.4758,  ...,  0.1343, -1.1484, -0.6132]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.3785,  0.0089,  0.0674,  ..., -0.2682,  1.4370,  1.6999],\n",
      "          [-0.1082,  0.5477,  0.3929,  ...,  1.7756, -0.6305, -0.7159]],\n",
      "\n",
      "         [[ 0.4789,  0.8376, -0.2197,  ...,  1.1087,  2.3084,  0.2624],\n",
      "          [ 1.0364,  2.6073, -0.5881,  ...,  1.7024, -0.9003, -0.1118]],\n",
      "\n",
      "         [[-0.0220,  1.9235, -0.3443,  ..., -0.3315,  2.1624,  0.9569],\n",
      "          [ 0.5618,  1.0137,  0.0877,  ...,  0.6882, -2.7216,  1.4170]]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[-0.4666,  0.1763, -0.0848,  ..., -0.2910,  0.0020, -0.0686],\n",
      "         [-0.1163,  0.1529, -0.2158,  ...,  0.5276,  0.5294,  0.3194],\n",
      "         [-0.5553, -0.0362,  0.0480,  ...,  0.0023, -0.0545,  0.2005],\n",
      "         ...,\n",
      "         [-0.7457,  0.1047, -0.4903,  ..., -0.0701,  0.0047, -0.0302],\n",
      "         [-0.5961, -0.1094,  0.0724,  ...,  0.2301,  0.1354,  0.2212],\n",
      "         [ 0.0595,  0.2151, -0.1316,  ...,  0.0089, -0.1633, -0.0060]],\n",
      "\n",
      "        [[-0.4666,  0.1763, -0.0848,  ..., -0.2910,  0.0020, -0.0686],\n",
      "         [-0.1869,  0.5488, -0.1860,  ..., -0.3346,  0.3373, -0.2549],\n",
      "         [-0.4024, -0.0347, -0.2437,  ..., -0.0822,  0.0555,  0.0304],\n",
      "         ...,\n",
      "         [-0.5297,  0.4794, -0.2098,  ...,  0.0160, -0.0135, -0.0717],\n",
      "         [-0.5296,  0.1584, -0.3855,  ...,  0.1938, -0.1130,  0.3601],\n",
      "         [ 0.0595,  0.2151, -0.1316,  ...,  0.0089, -0.1633, -0.0060]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[-0.4666,  0.1763, -0.0848,  ..., -0.2910,  0.0020, -0.0686],\n",
      "         [-0.1163,  0.1529, -0.2158,  ...,  0.5276,  0.5294,  0.3194],\n",
      "         [-0.5553, -0.0362,  0.0480,  ...,  0.0023, -0.0545,  0.2005],\n",
      "         ...,\n",
      "         [-0.7457,  0.1047, -0.4903,  ..., -0.0701,  0.0047, -0.0302],\n",
      "         [-0.5961, -0.1094,  0.0724,  ...,  0.2301,  0.1354,  0.2212],\n",
      "         [ 0.0595,  0.2151, -0.1316,  ...,  0.0089, -0.1633, -0.0060]],\n",
      "\n",
      "        [[-0.4666,  0.1763, -0.0848,  ..., -0.2910,  0.0020, -0.0686],\n",
      "         [-0.1869,  0.5488, -0.1860,  ..., -0.3346,  0.3373, -0.2549],\n",
      "         [-0.4024, -0.0347, -0.2437,  ..., -0.0822,  0.0555,  0.0304],\n",
      "         ...,\n",
      "         [-0.5297,  0.4794, -0.2098,  ...,  0.0160, -0.0135, -0.0717],\n",
      "         [-0.5296,  0.1584, -0.3855,  ...,  0.1938, -0.1130,  0.3601],\n",
      "         [ 0.0595,  0.2151, -0.1316,  ...,  0.0089, -0.1633, -0.0060]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[[-0.4666,  0.1763, -0.0848,  ...,  0.0053,  0.0988,  0.0014],\n",
      "          [-0.4882, -0.0838,  0.3034,  ..., -0.2910,  0.0020, -0.0686]],\n",
      "\n",
      "         [[-0.1163,  0.1529, -0.2158,  ...,  0.2286, -0.1955, -0.4021],\n",
      "          [-0.2209,  0.0069, -0.3868,  ...,  0.5276,  0.5294,  0.3194]],\n",
      "\n",
      "         [[-0.5553, -0.0362,  0.0480,  ...,  0.1075,  0.2148, -0.2127],\n",
      "          [-0.0851,  0.0094, -0.0710,  ...,  0.0023, -0.0545,  0.2005]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.7457,  0.1047, -0.4903,  ..., -0.2947, -0.2496, -0.1662],\n",
      "          [ 0.0137, -0.1850, -0.0697,  ..., -0.0701,  0.0047, -0.0302]],\n",
      "\n",
      "         [[-0.5961, -0.1094,  0.0724,  ...,  0.3084,  0.0757,  0.1235],\n",
      "          [-0.5564, -0.3991,  0.2341,  ...,  0.2301,  0.1354,  0.2212]],\n",
      "\n",
      "         [[ 0.0595,  0.2151, -0.1316,  ...,  0.3011,  0.0617,  0.2074],\n",
      "          [-0.6454,  0.0567, -0.3759,  ...,  0.0089, -0.1633, -0.0060]]],\n",
      "\n",
      "\n",
      "        [[[-0.4666,  0.1763, -0.0848,  ...,  0.0053,  0.0988,  0.0014],\n",
      "          [-0.4882, -0.0838,  0.3034,  ..., -0.2910,  0.0020, -0.0686]],\n",
      "\n",
      "         [[-0.1869,  0.5488, -0.1860,  ...,  0.2914, -0.2679,  0.1030],\n",
      "          [-1.0073,  0.0446, -0.3396,  ..., -0.3346,  0.3373, -0.2549]],\n",
      "\n",
      "         [[-0.4024, -0.0347, -0.2437,  ...,  0.1463, -0.0347, -0.2662],\n",
      "          [-0.4863,  0.0999,  0.0517,  ..., -0.0822,  0.0555,  0.0304]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.5297,  0.4794, -0.2098,  ...,  0.1366,  0.2343, -0.4197],\n",
      "          [-0.6107, -0.5613, -0.1616,  ...,  0.0160, -0.0135, -0.0717]],\n",
      "\n",
      "         [[-0.5296,  0.1584, -0.3855,  ...,  0.0175,  0.0208,  0.4507],\n",
      "          [-0.2018, -0.4820,  0.3206,  ...,  0.1938, -0.1130,  0.3601]],\n",
      "\n",
      "         [[ 0.0595,  0.2151, -0.1316,  ...,  0.3011,  0.0617,  0.2074],\n",
      "          [-0.6454,  0.0567, -0.3759,  ...,  0.0089, -0.1633, -0.0060]]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[[-4.6489e-01,  1.7417e-01, -8.3821e-02,  ...,  6.4759e-03,\n",
      "            9.9186e-02, -5.8158e-04],\n",
      "          [-1.1849e-01,  1.5139e-01, -2.1420e-01,  ...,  2.2760e-01,\n",
      "           -1.9353e-01, -3.9769e-01],\n",
      "          [-5.5364e-01, -3.5931e-02,  4.7244e-02,  ...,  1.0772e-01,\n",
      "            2.1309e-01, -2.1210e-01],\n",
      "          ...,\n",
      "          [-7.4483e-01,  1.0454e-01, -4.8958e-01,  ..., -2.9382e-01,\n",
      "           -2.4932e-01, -1.6586e-01],\n",
      "          [-5.9300e-01, -1.0788e-01,  7.0133e-02,  ...,  3.0642e-01,\n",
      "            7.3833e-02,  1.2166e-01],\n",
      "          [ 5.8229e-02,  2.1457e-01, -1.3136e-01,  ...,  3.0073e-01,\n",
      "            6.1792e-02,  2.0645e-01]],\n",
      "\n",
      "         [[-4.8753e-01, -8.3690e-02,  3.0219e-01,  ..., -2.9026e-01,\n",
      "            2.2764e-03, -6.7898e-02],\n",
      "          [-2.2118e-01,  6.6124e-03, -3.8585e-01,  ...,  5.2619e-01,\n",
      "            5.2835e-01,  3.1907e-01],\n",
      "          [-8.6759e-02,  9.8302e-03, -6.9809e-02,  ...,  1.4142e-03,\n",
      "           -5.3058e-02,  1.9914e-01],\n",
      "          ...,\n",
      "          [ 1.2135e-02, -1.8345e-01, -6.9839e-02,  ..., -6.9924e-02,\n",
      "            5.2658e-03, -2.9222e-02],\n",
      "          [-5.5559e-01, -3.9818e-01,  2.3316e-01,  ...,  2.2968e-01,\n",
      "            1.3549e-01,  2.2114e-01],\n",
      "          [-6.4053e-01,  5.6520e-02, -3.7217e-01,  ...,  7.5140e-03,\n",
      "           -1.6022e-01, -4.8655e-03]]],\n",
      "\n",
      "\n",
      "        [[[-4.6546e-01,  1.7452e-01, -8.5908e-02,  ...,  6.3044e-03,\n",
      "            9.7961e-02, -1.1121e-03],\n",
      "          [-1.8704e-01,  5.4701e-01, -1.8618e-01,  ...,  2.9104e-01,\n",
      "           -2.6700e-01,  1.0255e-01],\n",
      "          [-4.0196e-01, -3.3906e-02, -2.4360e-01,  ...,  1.4628e-01,\n",
      "           -3.4874e-02, -2.6526e-01],\n",
      "          ...,\n",
      "          [-5.2887e-01,  4.7783e-01, -2.0996e-01,  ...,  1.3655e-01,\n",
      "            2.3313e-01, -4.1833e-01],\n",
      "          [-5.2709e-01,  1.5829e-01, -3.8343e-01,  ...,  1.9086e-02,\n",
      "            2.0274e-02,  4.4396e-01],\n",
      "          [ 5.8655e-02,  2.1507e-01, -1.3176e-01,  ...,  3.0080e-01,\n",
      "            6.1218e-02,  2.0683e-01]],\n",
      "\n",
      "         [[-4.8822e-01, -8.3860e-02,  3.0257e-01,  ..., -2.9060e-01,\n",
      "            2.0670e-03, -6.8290e-02],\n",
      "          [-1.0055e+00,  4.4172e-02, -3.3892e-01,  ..., -3.3400e-01,\n",
      "            3.3608e-01, -2.5393e-01],\n",
      "          [-4.8647e-01,  9.8185e-02,  5.0358e-02,  ..., -8.2968e-02,\n",
      "            5.5098e-02,  3.0314e-02],\n",
      "          ...,\n",
      "          [-6.1025e-01, -5.6059e-01, -1.6148e-01,  ...,  1.5774e-02,\n",
      "           -1.3499e-02, -7.1521e-02],\n",
      "          [-2.0189e-01, -4.8156e-01,  3.2021e-01,  ...,  1.9338e-01,\n",
      "           -1.1287e-01,  3.5969e-01],\n",
      "          [-6.4441e-01,  5.5659e-02, -3.7323e-01,  ...,  7.1426e-03,\n",
      "           -1.6143e-01, -5.9186e-03]]]],\n",
      "       grad_fn=<ScaledDotProductFlashAttentionForCpuBackward0>)\n",
      "tensor([[[[-4.6489e-01,  1.7417e-01, -8.3821e-02,  ...,  6.4759e-03,\n",
      "            9.9186e-02, -5.8158e-04],\n",
      "          [-4.8753e-01, -8.3690e-02,  3.0219e-01,  ..., -2.9026e-01,\n",
      "            2.2764e-03, -6.7898e-02]],\n",
      "\n",
      "         [[-1.1849e-01,  1.5139e-01, -2.1420e-01,  ...,  2.2760e-01,\n",
      "           -1.9353e-01, -3.9769e-01],\n",
      "          [-2.2118e-01,  6.6124e-03, -3.8585e-01,  ...,  5.2619e-01,\n",
      "            5.2835e-01,  3.1907e-01]],\n",
      "\n",
      "         [[-5.5364e-01, -3.5931e-02,  4.7244e-02,  ...,  1.0772e-01,\n",
      "            2.1309e-01, -2.1210e-01],\n",
      "          [-8.6759e-02,  9.8302e-03, -6.9809e-02,  ...,  1.4142e-03,\n",
      "           -5.3058e-02,  1.9914e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-7.4483e-01,  1.0454e-01, -4.8958e-01,  ..., -2.9382e-01,\n",
      "           -2.4932e-01, -1.6586e-01],\n",
      "          [ 1.2135e-02, -1.8345e-01, -6.9839e-02,  ..., -6.9924e-02,\n",
      "            5.2658e-03, -2.9222e-02]],\n",
      "\n",
      "         [[-5.9300e-01, -1.0788e-01,  7.0133e-02,  ...,  3.0642e-01,\n",
      "            7.3833e-02,  1.2166e-01],\n",
      "          [-5.5559e-01, -3.9818e-01,  2.3316e-01,  ...,  2.2968e-01,\n",
      "            1.3549e-01,  2.2114e-01]],\n",
      "\n",
      "         [[ 5.8229e-02,  2.1457e-01, -1.3136e-01,  ...,  3.0073e-01,\n",
      "            6.1792e-02,  2.0645e-01],\n",
      "          [-6.4053e-01,  5.6520e-02, -3.7217e-01,  ...,  7.5140e-03,\n",
      "           -1.6022e-01, -4.8655e-03]]],\n",
      "\n",
      "\n",
      "        [[[-4.6546e-01,  1.7452e-01, -8.5908e-02,  ...,  6.3044e-03,\n",
      "            9.7961e-02, -1.1121e-03],\n",
      "          [-4.8822e-01, -8.3860e-02,  3.0257e-01,  ..., -2.9060e-01,\n",
      "            2.0670e-03, -6.8290e-02]],\n",
      "\n",
      "         [[-1.8704e-01,  5.4701e-01, -1.8618e-01,  ...,  2.9104e-01,\n",
      "           -2.6700e-01,  1.0255e-01],\n",
      "          [-1.0055e+00,  4.4172e-02, -3.3892e-01,  ..., -3.3400e-01,\n",
      "            3.3608e-01, -2.5393e-01]],\n",
      "\n",
      "         [[-4.0196e-01, -3.3906e-02, -2.4360e-01,  ...,  1.4628e-01,\n",
      "           -3.4874e-02, -2.6526e-01],\n",
      "          [-4.8647e-01,  9.8185e-02,  5.0358e-02,  ..., -8.2968e-02,\n",
      "            5.5098e-02,  3.0314e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-5.2887e-01,  4.7783e-01, -2.0996e-01,  ...,  1.3655e-01,\n",
      "            2.3313e-01, -4.1833e-01],\n",
      "          [-6.1025e-01, -5.6059e-01, -1.6148e-01,  ...,  1.5774e-02,\n",
      "           -1.3499e-02, -7.1521e-02]],\n",
      "\n",
      "         [[-5.2709e-01,  1.5829e-01, -3.8343e-01,  ...,  1.9086e-02,\n",
      "            2.0274e-02,  4.4396e-01],\n",
      "          [-2.0189e-01, -4.8156e-01,  3.2021e-01,  ...,  1.9338e-01,\n",
      "           -1.1287e-01,  3.5969e-01]],\n",
      "\n",
      "         [[ 5.8655e-02,  2.1507e-01, -1.3176e-01,  ...,  3.0080e-01,\n",
      "            6.1218e-02,  2.0683e-01],\n",
      "          [-6.4441e-01,  5.5659e-02, -3.7323e-01,  ...,  7.1426e-03,\n",
      "           -1.6143e-01, -5.9186e-03]]]], grad_fn=<TransposeBackward0>)\n",
      "tensor([[[-1.1548, -0.8279,  0.9725,  ...,  0.0294, -0.2039,  0.4724],\n",
      "         [-0.8616, -0.2664,  1.6752,  ...,  0.1661,  1.4628,  1.2761],\n",
      "         [-0.7292, -0.3559, -0.2388,  ..., -0.3661, -0.7655,  1.4221],\n",
      "         ...,\n",
      "         [-0.9380,  0.8399, -0.6693,  ...,  0.9383, -0.5113, -0.0460],\n",
      "         [-1.1300, -0.5631,  0.0294,  ...,  1.0096, -0.3012,  1.6072],\n",
      "         [-0.1300,  2.0359, -0.2869,  ...,  0.7407, -2.7933,  1.4546]],\n",
      "\n",
      "        [[-1.1552, -0.8274,  0.9710,  ...,  0.0292, -0.2041,  0.4721],\n",
      "         [-0.1805,  1.6129,  0.7737,  ..., -0.4215, -0.9359,  0.9491],\n",
      "         [-0.6895, -0.6821, -0.6010,  ..., -0.0108, -0.8126, -0.2970],\n",
      "         ...,\n",
      "         [-0.2297,  0.2126, -0.1375,  ...,  1.9284, -0.5461, -1.0998],\n",
      "         [-0.0937,  0.6369, -0.5729,  ...,  2.1472, -1.0905,  0.2577],\n",
      "         [-0.1296,  2.0361, -0.2869,  ...,  0.7403, -2.7941,  1.4535]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[[ 0.3152, -0.3397, -0.3717,  ..., -0.2119, -1.0830, -0.1619],\n",
      "         [ 0.2284, -0.0387, -0.1234,  ..., -0.4738, -0.0843,  0.2154],\n",
      "         [ 0.0147,  0.4299, -0.3423,  ..., -0.4652,  0.4800,  0.1883],\n",
      "         ...,\n",
      "         [ 0.5535, -0.2485,  0.5705,  ..., -0.1373, -0.1007, -0.4748],\n",
      "         [ 0.3080,  0.0809, -0.0870,  ..., -0.7118, -0.1247,  0.2769],\n",
      "         [ 0.8235, -0.1525,  0.1159,  ..., -0.0345,  0.3041, -0.1641]],\n",
      "\n",
      "        [[ 0.3153, -0.3401, -0.3714,  ..., -0.2118, -1.0831, -0.1620],\n",
      "         [ 0.3256, -0.1026,  0.3630,  ...,  0.0725, -0.1985, -0.3152],\n",
      "         [ 0.4808,  0.4720, -0.2882,  ..., -0.8577, -0.4574, -0.0513],\n",
      "         ...,\n",
      "         [ 0.6635, -0.1401,  0.3131,  ...,  0.2075, -0.1932,  0.0275],\n",
      "         [ 0.6458, -0.3223, -0.7408,  ...,  0.3540,  0.4483,  0.1675],\n",
      "         [ 0.8238, -0.1524,  0.1157,  ..., -0.0346,  0.3031, -0.1642]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[ 0.3152, -0.3397, -0.3717,  ..., -0.2119, -1.0830, -0.1619],\n",
      "         [ 0.2284, -0.0387, -0.1234,  ..., -0.4738, -0.0843,  0.2154],\n",
      "         [ 0.0147,  0.4299, -0.3423,  ..., -0.4652,  0.4800,  0.1883],\n",
      "         ...,\n",
      "         [ 0.5535, -0.2485,  0.5705,  ..., -0.1373, -0.1007, -0.4748],\n",
      "         [ 0.3080,  0.0809, -0.0870,  ..., -0.7118, -0.1247,  0.2769],\n",
      "         [ 0.8235, -0.1525,  0.1159,  ..., -0.0345,  0.3041, -0.1641]],\n",
      "\n",
      "        [[ 0.3153, -0.3401, -0.3714,  ..., -0.2118, -1.0831, -0.1620],\n",
      "         [ 0.3256, -0.1026,  0.3630,  ...,  0.0725, -0.1985, -0.3152],\n",
      "         [ 0.4808,  0.4720, -0.2882,  ..., -0.8577, -0.4574, -0.0513],\n",
      "         ...,\n",
      "         [ 0.6635, -0.1401,  0.3131,  ...,  0.2075, -0.1932,  0.0275],\n",
      "         [ 0.6458, -0.3223, -0.7408,  ...,  0.3540,  0.4483,  0.1675],\n",
      "         [ 0.8238, -0.1524,  0.1157,  ..., -0.0346,  0.3031, -0.1642]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[[ 0.3152, -0.3397, -0.3717,  ...,  0.0070, -1.4359, -1.8155],\n",
      "          [-0.4773,  0.2687,  0.2646,  ..., -0.2119, -1.0830, -0.1619]],\n",
      "\n",
      "         [[ 0.2284, -0.0387, -0.1234,  ..., -0.0655, -0.3390, -0.9916],\n",
      "          [ 0.3083,  0.0908,  0.5393,  ..., -0.4738, -0.0843,  0.2154]],\n",
      "\n",
      "         [[ 0.0147,  0.4299, -0.3423,  ...,  0.2785, -0.3981, -0.7898],\n",
      "          [-0.3717,  0.0447,  0.3824,  ..., -0.4652,  0.4800,  0.1883]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.5535, -0.2485,  0.5705,  ...,  0.4251, -0.4111, -0.5677],\n",
      "          [ 0.4316,  0.0660,  0.0971,  ..., -0.1373, -0.1007, -0.4748]],\n",
      "\n",
      "         [[ 0.3080,  0.0809, -0.0870,  ...,  0.1005, -0.6263, -0.5665],\n",
      "          [ 0.3775,  0.2828,  0.0639,  ..., -0.7118, -0.1247,  0.2769]],\n",
      "\n",
      "         [[ 0.8235, -0.1525,  0.1159,  ...,  0.4654, -0.9805, -1.0581],\n",
      "          [ 0.0029,  0.1440,  0.2397,  ..., -0.0345,  0.3041, -0.1641]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3153, -0.3401, -0.3714,  ...,  0.0068, -1.4358, -1.8155],\n",
      "          [-0.4772,  0.2687,  0.2648,  ..., -0.2118, -1.0831, -0.1620]],\n",
      "\n",
      "         [[ 0.3256, -0.1026,  0.3630,  ..., -0.0864, -0.4601, -0.7175],\n",
      "          [-0.0740, -0.0623,  0.2387,  ...,  0.0725, -0.1985, -0.3152]],\n",
      "\n",
      "         [[ 0.4808,  0.4720, -0.2882,  ...,  0.2039, -0.8566, -0.6162],\n",
      "          [ 0.3940,  0.6058, -0.0754,  ..., -0.8577, -0.4574, -0.0513]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.6635, -0.1401,  0.3131,  ...,  0.1487, -0.7738, -0.2336],\n",
      "          [ 0.1405,  0.0064,  0.2721,  ...,  0.2075, -0.1932,  0.0275]],\n",
      "\n",
      "         [[ 0.6458, -0.3223, -0.7408,  ...,  0.3833, -0.8987, -1.0069],\n",
      "          [-0.3980,  0.3708, -0.2330,  ...,  0.3540,  0.4483,  0.1675]],\n",
      "\n",
      "         [[ 0.8238, -0.1524,  0.1157,  ...,  0.4652, -0.9809, -1.0579],\n",
      "          [ 0.0028,  0.1435,  0.2397,  ..., -0.0346,  0.3031, -0.1642]]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[ 0.2513, -0.0742,  0.3245,  ..., -0.1566, -0.1570,  0.2999],\n",
      "         [-0.0140,  0.1582,  0.0266,  ..., -0.1526,  0.1407, -0.2266],\n",
      "         [ 0.0455, -0.1270,  0.4513,  ..., -0.5210, -0.4748, -0.1972],\n",
      "         ...,\n",
      "         [-0.5046, -0.1115,  0.3224,  ..., -0.3529,  0.1272, -0.5094],\n",
      "         [ 0.1632, -0.1699, -0.2103,  ...,  0.0471, -0.1441, -0.0456],\n",
      "         [-0.5903, -0.2938,  0.3024,  ...,  0.1319,  0.1227,  0.4514]],\n",
      "\n",
      "        [[ 0.2513, -0.0740,  0.3245,  ..., -0.1563, -0.1571,  0.2999],\n",
      "         [-0.0304, -0.1090,  0.0709,  ..., -0.2577,  0.4972, -0.2112],\n",
      "         [ 0.3651,  0.3258,  0.0152,  ..., -0.3502, -0.0556,  0.1308],\n",
      "         ...,\n",
      "         [-0.2512, -0.8886,  0.1190,  ...,  0.1255, -0.0502, -0.4423],\n",
      "         [-0.5021,  0.0930, -0.2460,  ..., -0.0365,  0.0600,  0.0108],\n",
      "         [-0.5900, -0.2936,  0.3027,  ...,  0.1320,  0.1227,  0.4512]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[ 0.2513, -0.0742,  0.3245,  ..., -0.1566, -0.1570,  0.2999],\n",
      "         [-0.0140,  0.1582,  0.0266,  ..., -0.1526,  0.1407, -0.2266],\n",
      "         [ 0.0455, -0.1270,  0.4513,  ..., -0.5210, -0.4748, -0.1972],\n",
      "         ...,\n",
      "         [-0.5046, -0.1115,  0.3224,  ..., -0.3529,  0.1272, -0.5094],\n",
      "         [ 0.1632, -0.1699, -0.2103,  ...,  0.0471, -0.1441, -0.0456],\n",
      "         [-0.5903, -0.2938,  0.3024,  ...,  0.1319,  0.1227,  0.4514]],\n",
      "\n",
      "        [[ 0.2513, -0.0740,  0.3245,  ..., -0.1563, -0.1571,  0.2999],\n",
      "         [-0.0304, -0.1090,  0.0709,  ..., -0.2577,  0.4972, -0.2112],\n",
      "         [ 0.3651,  0.3258,  0.0152,  ..., -0.3502, -0.0556,  0.1308],\n",
      "         ...,\n",
      "         [-0.2512, -0.8886,  0.1190,  ...,  0.1255, -0.0502, -0.4423],\n",
      "         [-0.5021,  0.0930, -0.2460,  ..., -0.0365,  0.0600,  0.0108],\n",
      "         [-0.5900, -0.2936,  0.3027,  ...,  0.1320,  0.1227,  0.4512]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[[ 0.2513, -0.0742,  0.3245,  ..., -0.0735,  0.3281, -0.1242],\n",
      "          [-0.1084, -0.2578,  0.0080,  ..., -0.1566, -0.1570,  0.2999]],\n",
      "\n",
      "         [[-0.0140,  0.1582,  0.0266,  ..., -0.3798, -0.3393, -0.3895],\n",
      "          [-0.1283,  0.1264, -0.2011,  ..., -0.1526,  0.1407, -0.2266]],\n",
      "\n",
      "         [[ 0.0455, -0.1270,  0.4513,  ...,  0.1180, -0.6914, -0.4560],\n",
      "          [ 0.0093,  0.2126,  0.7783,  ..., -0.5210, -0.4748, -0.1972]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.5046, -0.1115,  0.3224,  ...,  0.0732,  1.0299,  0.3147],\n",
      "          [ 0.3874, -0.8856,  0.0735,  ..., -0.3529,  0.1272, -0.5094]],\n",
      "\n",
      "         [[ 0.1632, -0.1699, -0.2103,  ..., -0.2050, -0.3292, -0.2022],\n",
      "          [-0.2335,  0.2259,  0.0682,  ...,  0.0471, -0.1441, -0.0456]],\n",
      "\n",
      "         [[-0.5903, -0.2938,  0.3024,  ..., -0.1860,  0.5833,  0.1515],\n",
      "          [-0.1169, -0.1959,  0.0924,  ...,  0.1319,  0.1227,  0.4514]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2513, -0.0740,  0.3245,  ..., -0.0733,  0.3283, -0.1242],\n",
      "          [-0.1082, -0.2581,  0.0077,  ..., -0.1563, -0.1571,  0.2999]],\n",
      "\n",
      "         [[-0.0304, -0.1090,  0.0709,  ...,  0.0489,  0.4491, -0.2012],\n",
      "          [ 0.1302,  0.1491,  0.0092,  ..., -0.2577,  0.4972, -0.2112]],\n",
      "\n",
      "         [[ 0.3651,  0.3258,  0.0152,  ..., -0.1549, -0.1433, -0.5152],\n",
      "          [ 0.2091,  0.0202,  0.1521,  ..., -0.3502, -0.0556,  0.1308]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.2512, -0.8886,  0.1190,  ..., -0.1279,  0.1108, -0.3233],\n",
      "          [ 0.4285, -0.0896,  0.0224,  ...,  0.1255, -0.0502, -0.4423]],\n",
      "\n",
      "         [[-0.5021,  0.0930, -0.2460,  ...,  0.0305,  0.0037, -0.1829],\n",
      "          [-0.0248, -0.1323,  0.2480,  ..., -0.0365,  0.0600,  0.0108]],\n",
      "\n",
      "         [[-0.5900, -0.2936,  0.3027,  ..., -0.1855,  0.5835,  0.1514],\n",
      "          [-0.1170, -0.1961,  0.0929,  ...,  0.1320,  0.1227,  0.4512]]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[-1.1548, -0.8279,  0.9725,  ...,  0.0294, -0.2039,  0.4724],\n",
      "         [-0.8616, -0.2664,  1.6752,  ...,  0.1661,  1.4628,  1.2761],\n",
      "         [-0.7292, -0.3559, -0.2388,  ..., -0.3661, -0.7655,  1.4221],\n",
      "         ...,\n",
      "         [-0.9380,  0.8399, -0.6693,  ...,  0.9383, -0.5113, -0.0460],\n",
      "         [-1.1300, -0.5631,  0.0294,  ...,  1.0096, -0.3012,  1.6072],\n",
      "         [-0.1300,  2.0359, -0.2869,  ...,  0.7407, -2.7933,  1.4546]],\n",
      "\n",
      "        [[-1.1552, -0.8274,  0.9710,  ...,  0.0292, -0.2041,  0.4721],\n",
      "         [-0.1805,  1.6129,  0.7737,  ..., -0.4215, -0.9359,  0.9491],\n",
      "         [-0.6895, -0.6821, -0.6010,  ..., -0.0108, -0.8126, -0.2970],\n",
      "         ...,\n",
      "         [-0.2297,  0.2126, -0.1375,  ...,  1.9284, -0.5461, -1.0998],\n",
      "         [-0.0937,  0.6369, -0.5729,  ...,  2.1472, -1.0905,  0.2577],\n",
      "         [-0.1296,  2.0361, -0.2869,  ...,  0.7403, -2.7941,  1.4535]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[[-1.1548, -0.8279,  0.9725,  ...,  0.0294, -0.2039,  0.4724],\n",
      "         [-0.8616, -0.2664,  1.6752,  ...,  0.1661,  1.4628,  1.2761],\n",
      "         [-0.7292, -0.3559, -0.2388,  ..., -0.3661, -0.7655,  1.4221],\n",
      "         ...,\n",
      "         [-0.9380,  0.8399, -0.6693,  ...,  0.9383, -0.5113, -0.0460],\n",
      "         [-1.1300, -0.5631,  0.0294,  ...,  1.0096, -0.3012,  1.6072],\n",
      "         [-0.1300,  2.0359, -0.2869,  ...,  0.7407, -2.7933,  1.4546]],\n",
      "\n",
      "        [[-1.1552, -0.8274,  0.9710,  ...,  0.0292, -0.2041,  0.4721],\n",
      "         [-0.1805,  1.6129,  0.7737,  ..., -0.4215, -0.9359,  0.9491],\n",
      "         [-0.6895, -0.6821, -0.6010,  ..., -0.0108, -0.8126, -0.2970],\n",
      "         ...,\n",
      "         [-0.2297,  0.2126, -0.1375,  ...,  1.9284, -0.5461, -1.0998],\n",
      "         [-0.0937,  0.6369, -0.5729,  ...,  2.1472, -1.0905,  0.2577],\n",
      "         [-0.1296,  2.0361, -0.2869,  ...,  0.7403, -2.7941,  1.4535]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[[[-1.1548, -0.8279,  0.9725,  ..., -0.2104,  1.7520,  1.3442],\n",
      "          [-0.5062,  0.9147,  1.3157,  ...,  0.0294, -0.2039,  0.4724]],\n",
      "\n",
      "         [[-0.8616, -0.2664,  1.6752,  ...,  0.1002,  0.1183, -0.8525],\n",
      "          [-0.2860,  0.8652,  1.0637,  ...,  0.1661,  1.4628,  1.2761]],\n",
      "\n",
      "         [[-0.7292, -0.3559, -0.2388,  ...,  0.7860, -0.7463, -0.5850],\n",
      "          [ 0.7732, -1.1661,  0.8310,  ..., -0.3661, -0.7655,  1.4221]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.9380,  0.8399, -0.6693,  ...,  0.5893, -0.0041,  1.9046],\n",
      "          [ 0.3326,  0.1972,  0.1391,  ...,  0.9383, -0.5113, -0.0460]],\n",
      "\n",
      "         [[-1.1300, -0.5631,  0.0294,  ...,  0.7024, -0.0719,  0.9345],\n",
      "          [ 0.6283,  1.3279, -0.7127,  ...,  1.0096, -0.3012,  1.6072]],\n",
      "\n",
      "         [[-0.1300,  2.0359, -0.2869,  ..., -0.2332,  2.1096,  1.4361],\n",
      "          [-0.2186,  0.8912,  0.1668,  ...,  0.7407, -2.7933,  1.4546]]],\n",
      "\n",
      "\n",
      "        [[[-1.1552, -0.8274,  0.9710,  ..., -0.2102,  1.7510,  1.3435],\n",
      "          [-0.5067,  0.9143,  1.3159,  ...,  0.0292, -0.2041,  0.4721]],\n",
      "\n",
      "         [[-0.1805,  1.6129,  0.7737,  ..., -0.6810, -0.8857,  1.2150],\n",
      "          [-1.3090,  0.4387, -0.0804,  ..., -0.4215, -0.9359,  0.9491]],\n",
      "\n",
      "         [[-0.6895, -0.6821, -0.6010,  ...,  0.5249,  0.0892,  0.2671],\n",
      "          [ 0.6354, -0.0196,  0.9019,  ..., -0.0108, -0.8126, -0.2970]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.2297,  0.2126, -0.1375,  ..., -0.0596,  1.3631,  1.3052],\n",
      "          [-0.9864, -0.2573,  0.1976,  ...,  1.9284, -0.5461, -1.0998]],\n",
      "\n",
      "         [[-0.0937,  0.6369, -0.5729,  ...,  1.1112,  2.1957,  0.7773],\n",
      "          [ 0.5986,  1.9021, -0.2708,  ...,  2.1472, -1.0905,  0.2577]],\n",
      "\n",
      "         [[-0.1296,  2.0361, -0.2869,  ..., -0.2331,  2.1087,  1.4362],\n",
      "          [-0.2225,  0.8901,  0.1657,  ...,  0.7403, -2.7941,  1.4535]]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[[-0.9987, -0.5693,  0.3722,  ..., -0.3505,  0.0201,  0.2872],\n",
      "          [-0.8318, -0.3636,  0.3083,  ..., -0.0279,  0.4072,  0.5141],\n",
      "          [-0.8372, -0.3696,  0.3055,  ..., -0.0460,  0.3689,  0.5067],\n",
      "          ...,\n",
      "          [-0.7967, -0.3004,  0.2688,  ...,  0.0136,  0.5528,  0.5904],\n",
      "          [-0.8246, -0.3843,  0.2899,  ..., -0.0428,  0.3709,  0.5166],\n",
      "          [-0.8640, -0.4144,  0.3205,  ..., -0.0544,  0.2957,  0.4376]],\n",
      "\n",
      "         [[-0.1179,  0.8541,  0.4597,  ..., -0.0465,  0.6447,  0.9345],\n",
      "          [-0.1232,  0.7586,  0.4778,  ...,  0.1942, -0.1360,  0.8755],\n",
      "          [-0.1449,  0.7978,  0.4712,  ...,  0.1783, -0.0673,  0.8698],\n",
      "          ...,\n",
      "          [-0.1573,  0.7521,  0.5033,  ...,  0.2330, -0.2793,  0.8364],\n",
      "          [-0.1431,  0.7521,  0.5082,  ...,  0.1924, -0.1673,  0.8796],\n",
      "          [-0.1090,  0.7607,  0.4772,  ...,  0.1569,  0.0025,  0.9043]]],\n",
      "\n",
      "\n",
      "        [[[-0.0105, -0.7273, -0.4469,  ...,  1.2360,  0.7369,  0.5954],\n",
      "          [-0.2448, -0.3248, -0.2671,  ...,  0.6850,  0.8796,  0.7811],\n",
      "          [-0.1947, -0.3815, -0.2934,  ...,  0.8018,  0.8631,  0.7517],\n",
      "          ...,\n",
      "          [-0.2329, -0.2430, -0.2634,  ...,  0.6588,  0.8692,  0.7989],\n",
      "          [-0.1904, -0.3808, -0.2997,  ...,  0.7768,  0.8546,  0.7598],\n",
      "          [-0.2132, -0.4405, -0.3212,  ...,  0.8038,  0.8184,  0.7192]],\n",
      "\n",
      "         [[-0.2121,  0.6609,  0.1014,  ...,  0.3021, -0.6543,  0.1034],\n",
      "          [-0.3258,  0.5917,  0.2079,  ...,  0.4307, -0.7291,  0.2084],\n",
      "          [-0.3148,  0.5959,  0.1495,  ...,  0.3806, -0.6651,  0.1680],\n",
      "          ...,\n",
      "          [-0.3593,  0.5983,  0.1843,  ...,  0.4551, -0.7767,  0.2434],\n",
      "          [-0.2947,  0.5987,  0.1705,  ...,  0.4396, -0.7160,  0.1743],\n",
      "          [-0.2735,  0.5878,  0.2079,  ...,  0.4331, -0.7097,  0.1586]]]],\n",
      "       grad_fn=<ScaledDotProductFlashAttentionForCpuBackward0>)\n",
      "tensor([[[[-0.9987, -0.5693,  0.3722,  ..., -0.3505,  0.0201,  0.2872],\n",
      "          [-0.1179,  0.8541,  0.4597,  ..., -0.0465,  0.6447,  0.9345]],\n",
      "\n",
      "         [[-0.8318, -0.3636,  0.3083,  ..., -0.0279,  0.4072,  0.5141],\n",
      "          [-0.1232,  0.7586,  0.4778,  ...,  0.1942, -0.1360,  0.8755]],\n",
      "\n",
      "         [[-0.8372, -0.3696,  0.3055,  ..., -0.0460,  0.3689,  0.5067],\n",
      "          [-0.1449,  0.7978,  0.4712,  ...,  0.1783, -0.0673,  0.8698]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.7967, -0.3004,  0.2688,  ...,  0.0136,  0.5528,  0.5904],\n",
      "          [-0.1573,  0.7521,  0.5033,  ...,  0.2330, -0.2793,  0.8364]],\n",
      "\n",
      "         [[-0.8246, -0.3843,  0.2899,  ..., -0.0428,  0.3709,  0.5166],\n",
      "          [-0.1431,  0.7521,  0.5082,  ...,  0.1924, -0.1673,  0.8796]],\n",
      "\n",
      "         [[-0.8640, -0.4144,  0.3205,  ..., -0.0544,  0.2957,  0.4376],\n",
      "          [-0.1090,  0.7607,  0.4772,  ...,  0.1569,  0.0025,  0.9043]]],\n",
      "\n",
      "\n",
      "        [[[-0.0105, -0.7273, -0.4469,  ...,  1.2360,  0.7369,  0.5954],\n",
      "          [-0.2121,  0.6609,  0.1014,  ...,  0.3021, -0.6543,  0.1034]],\n",
      "\n",
      "         [[-0.2448, -0.3248, -0.2671,  ...,  0.6850,  0.8796,  0.7811],\n",
      "          [-0.3258,  0.5917,  0.2079,  ...,  0.4307, -0.7291,  0.2084]],\n",
      "\n",
      "         [[-0.1947, -0.3815, -0.2934,  ...,  0.8018,  0.8631,  0.7517],\n",
      "          [-0.3148,  0.5959,  0.1495,  ...,  0.3806, -0.6651,  0.1680]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.2329, -0.2430, -0.2634,  ...,  0.6588,  0.8692,  0.7989],\n",
      "          [-0.3593,  0.5983,  0.1843,  ...,  0.4551, -0.7767,  0.2434]],\n",
      "\n",
      "         [[-0.1904, -0.3808, -0.2997,  ...,  0.7768,  0.8546,  0.7598],\n",
      "          [-0.2947,  0.5987,  0.1705,  ...,  0.4396, -0.7160,  0.1743]],\n",
      "\n",
      "         [[-0.2132, -0.4405, -0.3212,  ...,  0.8038,  0.8184,  0.7192],\n",
      "          [-0.2735,  0.5878,  0.2079,  ...,  0.4331, -0.7097,  0.1586]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'by'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(\n\u001b[1;32m      2\u001b[0m     direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     study_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert-tiny-nas-study\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m     sampler\u001b[38;5;241m=\u001b[39msampler\n\u001b[1;32m      5\u001b[0m )\n\u001b[0;32m----> 7\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpipeObjective\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m24\u001b[39;49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/mase_env/lib/python3.11/site-packages/optuna/study/study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/mase_env/lib/python3.11/site-packages/optuna/study/_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniforge3/envs/mase_env/lib/python3.11/site-packages/optuna/study/_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/miniforge3/envs/mase_env/lib/python3.11/site-packages/optuna/study/_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    247\u001b[0m ):\n\u001b[0;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/miniforge3/envs/mase_env/lib/python3.11/site-packages/optuna/study/_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[33], line 31\u001b[0m, in \u001b[0;36mpipeObjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     28\u001b[0m qc \u001b[38;5;241m=\u001b[39m quantization_config\n\u001b[1;32m     29\u001b[0m pc \u001b[38;5;241m=\u001b[39m pruning_config\n\u001b[0;32m---> 31\u001b[0m mg, _ \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpass_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquantize_transform_pass\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mqc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprune_transform_pass\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m trainer \u001b[38;5;241m=\u001b[39m get_trainer(\n\u001b[1;32m     40\u001b[0m     model \u001b[38;5;241m=\u001b[39m model,\n\u001b[1;32m     41\u001b[0m     tokenized_dataset \u001b[38;5;241m=\u001b[39m dataset,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     44\u001b[0m     num_train_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     45\u001b[0m )\n\u001b[1;32m     47\u001b[0m post_comp_no_t \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate()\n",
      "File \u001b[0;32m~/mase/src/chop/pipelines/auto_pipeline.py:72\u001b[0m, in \u001b[0;36mAutoPipeline.__call__\u001b[0;34m(self, mg, pass_args, skip_passes)\u001b[0m\n\u001b[1;32m     67\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning pass group \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpass_groups)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     68\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m     69\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following passes will be executed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m[pass_fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mpass_fn\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mpass_group]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     70\u001b[0m     )\n\u001b[0;32m---> 72\u001b[0m     mg, pass_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_pass_group\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpass_group\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpass_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskip_passes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpass_outputs[idx] \u001b[38;5;241m=\u001b[39m pass_outputs\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mg, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpass_outputs\n",
      "File \u001b[0;32m~/mase/src/chop/pipelines/auto_pipeline.py:53\u001b[0m, in \u001b[0;36mAutoPipeline._run_pass_group\u001b[0;34m(self, mg, pass_group, pass_args, skip_passes)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m v\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself/\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     51\u001b[0m             args[k] \u001b[38;5;241m=\u001b[39m pass_outputs[v[\u001b[38;5;241m5\u001b[39m:]]\n\u001b[0;32m---> 53\u001b[0m     mg, pass_output \u001b[38;5;241m=\u001b[39m \u001b[43mpass_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpass_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     pass_outputs[pass_fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m] \u001b[38;5;241m=\u001b[39m pass_output\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mg, pass_outputs\n",
      "File \u001b[0;32m~/mase/src/chop/passes/graph/transforms/quantize/quantize.py:248\u001b[0m, in \u001b[0;36mquantize_transform_pass\u001b[0;34m(graph, pass_args)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mquantize_transform_pass\u001b[39m(graph, pass_args\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    209\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;124;03m    Apply quantization transformation to the given graph.\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;124;03m        - by -> str : different quantization schemes choose from [\"type\", \"name\", \"regx_name\"]\u001b[39;00m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 248\u001b[0m     by \u001b[38;5;241m=\u001b[39m \u001b[43mpass_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mby\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mmatch\u001b[39;00m by:\n\u001b[1;32m    250\u001b[0m         \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'by'"
     ]
    }
   ],
   "source": [
    "\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    study_name=f\"bert-tiny-nas-study\",\n",
    "    sampler=sampler\n",
    ")\n",
    "\n",
    "study.optimize(\n",
    "    pipeObjective,\n",
    "    n_trials=1,\n",
    "    timeout=60*60*24\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mase_env]",
   "language": "python",
   "name": "conda-env-mase_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
